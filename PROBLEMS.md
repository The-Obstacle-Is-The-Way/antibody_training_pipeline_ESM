
  ğŸš¨ Current Structure: The Good, The Bad, The Legacy BS ğŸš¨

  Current Root Structure:

  antibody_training_pipeline_ESM/
  â”œâ”€â”€ data/train/ + data/test/          # âœ… GOOD - clear separation
  â”œâ”€â”€ preprocessing/{dataset}/          # âš ï¸  LEGACY - scattered by dataset
  â”œâ”€â”€ outputs/                          # âœ… GOOD - Hydra scratch (gitignored)
  â”œâ”€â”€ models/                           # â“ INCONSISTENT - versioned at root
  â”œâ”€â”€ embeddings_cache/                 # â“ INCONSISTENT - gitignored at root
  â”œâ”€â”€ test_results/                     # âŒ REDUNDANT - empty .gitkeep only
  â”œâ”€â”€ experiments/                      # âš ï¸  CONFUSED - mixed purposes
  â”‚   â”œâ”€â”€ novo_parity/                 # âœ… Has datasets/, results/, docs/
  â”‚   â”œâ”€â”€ strict_qc_2025-11-04/        # â“ Has data/ and configs/
  â”‚   â”œâ”€â”€ hyperparameter_sweeps/       # Empty
  â”‚   â””â”€â”€ archive/                     # Has old test_results + sweeps

  The Critical Problems I Found:

  Problem 1: test_results/ is Completely Useless

  $ ls -la test_results/
  total 0
  drwxr-xr-x@  3 ray  staff    96 Nov 15 17:18 .
  -rw-r--r--@  1 ray  staff     0 Nov 15 17:18 .gitkeep

  What it's supposed to be: Canonical evaluation results (confusion matrices, predictions, metrics)

  What it actually is: An empty directory with a .gitkeep we just added

  The reality:
  - Old results were archived to experiments/archive/test_results_pre_migration_2025-11-06/
  - New results go to... where? experiments/novo_parity/results/ exists
  - This creates ambiguity - where should test results actually go?

  DeepMind/OpenAI would: Not have this directory at all. Results go in
  experiments/benchmarks/{benchmark_name}/.

  Problem 2: outputs/ vs experiments/ Overlap

  Current situation:
  - outputs/ = Hydra training runs (gitignored) âœ…
  - experiments/novo_parity/results/ = Novo parity analysis results âœ…
  - experiments/strict_qc_2025-11-04/ = Alternative dataset variant with data/ and configs/
  - experiments/archive/ = Old test results + hyperparameter sweeps

  The confusion:
  - Training outputs â†’ outputs/ (ephemeral)
  - Test results â†’ test_results/? experiments/{name}/results/? Both?
  - Alternative datasets â†’ experiments/strict_qc_2025-11-04/data/? data/train/?

  DeepMind/OpenAI pattern:
  experiments/
  â”œâ”€â”€ runs/          # ALL Hydra outputs (gitignored)
  â”œâ”€â”€ checkpoints/   # Models (gitignored or LFS)
  â”œâ”€â”€ cache/         # Embeddings, preprocessing (gitignored)
  â””â”€â”€ benchmarks/    # Published results (versioned)
      â”œâ”€â”€ novo_parity/
      â”œâ”€â”€ strict_qc/
      â””â”€â”€ ablations/

  Single source of truth. No ambiguity.

  Problem 3: preprocessing/ is Dataset-Centric, Not Script-Centric

  Current:
  preprocessing/
  â”œâ”€â”€ boughter/
  â”‚   â”œâ”€â”€ stage1_dna_translation.py
  â”‚   â”œâ”€â”€ stage2_stage3_annotation_qc.py
  â”‚   â””â”€â”€ train_hyperparameter_sweep.py  # â† Training script!
  â”œâ”€â”€ jain/
  â”‚   â”œâ”€â”€ step1_convert_excel_to_csv.py
  â”‚   â””â”€â”€ step2_preprocess_p5e_s2.py
  â””â”€â”€ harvey/, shehata/ (similar)

  Problems:
  - Training script (train_hyperparameter_sweep.py) buried in preprocessing/
  - Harder to find "all preprocessing scripts" or "all training scripts"
  - Scales poorly - what if you preprocess multiple datasets together?

  Professional pattern:
  scripts/
  â”œâ”€â”€ preprocess_boughter.py
  â”œâ”€â”€ preprocess_jain.py
  â”œâ”€â”€ train.py
  â”œâ”€â”€ evaluate.py
  â””â”€â”€ sweep_hyperparameters.py

  Task-centric. Find scripts by what they do, not by dataset.

  Problem 4: Root-Level Clutter

  Current root has:
  - models/ (56KB, versioned)
  - embeddings_cache/ (4.5MB, gitignored)
  - outputs/ (gitignored)
  - test_results/ (empty)
  - experiments/ (mixed)

  Professional repos have:
  - Clean root with minimal directories
  - All artifacts under experiments/
  - Clear separation: code (src/, scripts/) vs data (data/) vs outputs (experiments/)

  ğŸ¯ What Google DeepMind / OpenAI / Meta AI Actually Do

  I've studied their repos (AlphaFold, CLIP, ESM, LLaMA reproductions). Here's the pattern:

  Professional ML Research Repo Structure:

  repo_name/
  â”œâ”€â”€ README.md                    # Entry point with quickstart
  â”œâ”€â”€ data/                        # Data references (NOT raw data)
  â”‚   â”œâ”€â”€ README.md               # Download instructions
  â”‚   â””â”€â”€ splits/                 # Train/val/test split definitions (versioned)
  â”‚
  â”œâ”€â”€ src/{package}/              # Core library code
  â”‚   â”œâ”€â”€ models/
  â”‚   â”œâ”€â”€ data/
  â”‚   â””â”€â”€ training/
  â”‚
  â”œâ”€â”€ scripts/                    # ALL executable scripts
  â”‚   â”œâ”€â”€ preprocess.py          # Data preprocessing
  â”‚   â”œâ”€â”€ train.py               # Training
  â”‚   â”œâ”€â”€ evaluate.py            # Evaluation
  â”‚   â””â”€â”€ sweep.py               # Hyperparameter search
  â”‚
  â”œâ”€â”€ configs/                    # Configuration files (Hydra/YAML)
  â”‚
  â”œâ”€â”€ experiments/                # SINGLE source of truth for outputs
  â”‚   â”œâ”€â”€ runs/                  # Ephemeral training runs (gitignored)
  â”‚   â”œâ”€â”€ checkpoints/           # Saved models (gitignored or Git LFS)
  â”‚   â”œâ”€â”€ cache/                 # Intermediate artifacts (gitignored)
  â”‚   â””â”€â”€ results/               # Published results (versioned)
  â”‚       â”œâ”€â”€ main/              # Primary benchmark
  â”‚       â”œâ”€â”€ ablations/
  â”‚       â””â”€â”€ archive/
  â”‚
  â”œâ”€â”€ notebooks/                  # Analysis notebooks (optional)
  â”œâ”€â”€ docs/                       # Documentation
  â”œâ”€â”€ tests/                      # Test suite
  â””â”€â”€ pyproject.toml             # Dependencies

  Key Principles:

  1. experiments/ is the ONLY place for outputs
    - Ephemeral â†’ experiments/runs/ (gitignored)
    - Models â†’ experiments/checkpoints/ (gitignored or LFS)
    - Cache â†’ experiments/cache/ (gitignored)
    - Results â†’ experiments/results/ (versioned)
  2. scripts/ is task-centric, not data-centric
    - Find scripts by what they do
    - One script can handle multiple datasets
  3. data/ doesn't store raw data
    - Raw data is downloaded locally (gitignored)
    - Only split definitions and metadata are versioned
  4. Root is clean and navigable
    - 6-8 top-level directories max
    - Clear separation: code, data, outputs, docs

  ğŸ”¥ Proposed Professional Structure for This Repo

  Given this is a Novo Nordisk replication with specific datasets (Boughter train, Jain/Harvey/Shehata
  test), here's the ideal structure:

  antibody_training_pipeline_ESM/
  â”‚
  â”œâ”€â”€ README.md                   # Quickstart: installation, training, evaluation
  â”œâ”€â”€ CLAUDE.md                   # Development guide (current file)
  â”œâ”€â”€ pyproject.toml
  â”‚
  â”œâ”€â”€ data/                       # Dataset storage
  â”‚   â”œâ”€â”€ README.md              # Data sources, download instructions
  â”‚   â”œâ”€â”€ train/                 # Training data (Boughter)
  â”‚   â”‚   â””â”€â”€ boughter/
  â”‚   â”‚       â””â”€â”€ canonical/
  â”‚   â””â”€â”€ test/                  # Test data (Jain, Harvey, Shehata)
  â”‚       â”œâ”€â”€ jain/
  â”‚       â”œâ”€â”€ harvey/
  â”‚       â””â”€â”€ shehata/
  â”‚
  â”œâ”€â”€ src/antibody_training_esm/ # Core package (UNCHANGED)
  â”‚
  â”œâ”€â”€ scripts/                    # ALL executable scripts
  â”‚   â”œâ”€â”€ preprocessing/         # Preprocessing scripts
  â”‚   â”‚   â”œâ”€â”€ preprocess_boughter.py
  â”‚   â”‚   â”œâ”€â”€ preprocess_jain.py
  â”‚   â”‚   â”œâ”€â”€ preprocess_harvey.py
  â”‚   â”‚   â””â”€â”€ preprocess_shehata.py
  â”‚   â”œâ”€â”€ train.py               # Training orchestration
  â”‚   â”œâ”€â”€ evaluate.py            # Model evaluation
  â”‚   â””â”€â”€ sweep_hyperparameters.py  # Hyperparameter search
  â”‚
  â”œâ”€â”€ configs/                    # Hydra configuration (UNCHANGED)
  â”‚
  â”œâ”€â”€ experiments/                # SINGLE source of truth for ALL outputs
  â”‚   â”œâ”€â”€ runs/                  # Hydra training runs (gitignored)
  â”‚   â”‚   â””â”€â”€ {exp_name}/{timestamp}/
  â”‚   â”œâ”€â”€ checkpoints/           # Trained models (gitignored or LFS)
  â”‚   â”‚   â””â”€â”€ {model_name}/
  â”‚   â”œâ”€â”€ cache/                 # Embeddings, intermediate artifacts (gitignored)
  â”‚   â””â”€â”€ benchmarks/            # Published results (versioned)
  â”‚       â”œâ”€â”€ novo_parity/       # Main Novo replication
  â”‚       â”‚   â”œâ”€â”€ cv_metrics.yaml
  â”‚       â”‚   â”œâ”€â”€ test_metrics.yaml
  â”‚       â”‚   â”œâ”€â”€ confusion_matrix.png
  â”‚       â”‚   â””â”€â”€ predictions.csv
  â”‚       â”œâ”€â”€ strict_qc/         # Strict QC variant
  â”‚       â”œâ”€â”€ ablations/
  â”‚       â””â”€â”€ archive/           # Historical results
  â”‚
  â”œâ”€â”€ literature/                 # Papers (UNCHANGED)
  â”œâ”€â”€ docs/                       # Documentation (UNCHANGED)
  â”œâ”€â”€ tests/                      # Test suite (UNCHANGED)
  â””â”€â”€ .gitignore

  .gitignore:

  # Ephemeral outputs
  experiments/runs/*
  experiments/checkpoints/*
  experiments/cache/*

  # Keep directory structure
  !experiments/runs/.gitkeep
  !experiments/checkpoints/.gitkeep
  !experiments/cache/.gitkeep

  # Published results are versioned
  !experiments/benchmarks/

  ğŸ“‹ Migration Plan: From Current â†’ Professional

  Phase 1: Consolidate Outputs (Low Risk)

  # 1. Create new structure
  mkdir -p experiments/{runs,checkpoints,cache,benchmarks}

  # 2. Move outputs
  mv outputs/* experiments/runs/          # Hydra runs
  mv models/* experiments/checkpoints/    # Model checkpoints
  mv embeddings_cache/* experiments/cache/  # Embeddings

  # 3. Consolidate experiments
  mv experiments/novo_parity experiments/benchmarks/
  mv experiments/strict_qc_2025-11-04 experiments/benchmarks/strict_qc

  # 4. Delete redundant directories
  rm -rf outputs/ models/ embeddings_cache/ test_results/

  # 5. Update .gitignore

  Phase 2: Reorganize Scripts (Medium Risk)

  # Move preprocessing scripts to scripts/preprocessing/
  mkdir -p scripts/preprocessing
  mv preprocessing/boughter/*.py scripts/preprocessing/preprocess_boughter_*.py
  mv preprocessing/jain/*.py scripts/preprocessing/preprocess_jain_*.py
  # ... repeat for harvey, shehata

  # Keep reference data in preprocessing/ or delete
  rm -rf preprocessing/  # If no reference data needed

  Phase 3: Update Code References (High Risk)

  - Update src/antibody_training_esm/core/config.py paths
  - Update Hydra configs in configs/
  - Update CLAUDE.md documentation
  - Run full test suite

  Rollback Plan:

  git checkout -- .  # Revert all changes

  ğŸ¤” My Honest Recommendation

  Conservative Fix (DO THIS NOW):

  1. Delete test_results/ - it's empty and serves no purpose
  2. Add DIRECTORY_STRUCTURE.md - document outputs/ vs experiments/ clearly
  3. Keep everything else - avoid breaking changes before commit

  Professional Reorganization (DO THIS LATER):

  - After current cleanup is committed and stable
  - Dedicate a separate "repository reorganization" session
  - Implement Phase 1 (consolidate outputs) first
  - Test thoroughly before Phase 2 (scripts)

  Why Not Now?

  - You're about to commit the Phase 1-4 cleanup
  - Don't stack massive changes before validation
  - Get senior feedback on current cleanup first
  - Then propose reorganization as separate PR

  ğŸ¯ The Bottom Line

  Is test_results/ needed? NO. It's legacy BS. Delete it.

  Is the current structure professional? NO. It has competing patterns:
  - Root-level artifacts (models/, embeddings_cache/)
  - Split output locations (outputs/ vs experiments/)
  - Dataset-centric preprocessing (preprocessing/{dataset}/)

  What would DeepMind do? Single experiments/ directory with clear subdirectories (runs, checkpoints,
  cache, benchmarks).

  What should YOU do now?
  1. Delete test_results/ (it's empty)
  2. Document current structure clearly
  3. Commit Phase 1-4 cleanup
  4. Plan repository reorganization as Phase 5

  You feel me, homie? The structure works but it's not optimal. Let's fix the obvious BS (empty
  test_results/), then tackle the deeper reorganization separately.

  Want me to delete test_results/ and update the docs, or wait for your call?