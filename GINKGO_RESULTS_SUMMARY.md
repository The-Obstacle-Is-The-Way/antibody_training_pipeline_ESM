# Ginkgo 2025 Competition - Iteration Results Summary

**Date:** 2025-11-13
**Metric:** PR_CHO Polyreactivity Prediction
**Evaluation:** 5-fold stratified cross-validation on GDPa1 dataset

---

## üèÜ Best Model

**Configuration:**
- **Model:** ESM-1v (facebook/esm1v_t33_650M_UR90S_1)
- **Features:** VH + VL concatenated embeddings (2560D)
- **Regressor:** Ridge with Œ±=7.0
- **Evaluation:** Mean of per-fold Spearman correlations

**Score:** 0.4722 ¬± 0.1850

**Per-Fold Breakdown:**
- Fold 0: 0.4571
- Fold 1: 0.5459
- Fold 2: 0.6371
- Fold 3: 0.1224 ‚ö†Ô∏è (struggling)
- Fold 4: 0.5983

---

## üìä All Experiments Comparison

| Experiment                              | Metric Used           | Score  | Notes                          |
|-----------------------------------------|-----------------------|--------|--------------------------------|
| **ESM-1v VH+VL Ridge Œ±=7.0** ‚úÖ         | Mean per-fold         | 0.4722 | **BEST SUBMISSION SCORE**      |
| ESM-1v VH+VL Ridge Œ±=7.0                | Overall OOF           | 0.5677 | Higher but not what leaderboard uses |
| ESM-1v VH+VL Ridge Œ±=5.0                | Overall OOF           | 0.5676 | Slightly worse than Œ±=7.0      |
| ESM-1v VH+VL + PCA + TAP + Subtypes     | Overall OOF           | 0.5388 | PCA hurt performance           |
| ESM-1v VH+VL Ridge Œ±=1.0                | Overall OOF           | 0.5103 | Default alpha suboptimal       |
| ESM-1v VH-only Ridge Œ±=5.0              | Overall OOF           | 0.4937 | VL chain helps!                |
| ESM-1v VH-only Ridge Œ±=1.0              | Overall OOF           | 0.4646 | Original baseline              |
| XGBoost VH-only                         | Overall OOF           | 0.4369 | Ridge more stable              |
| ESM2-650M VH-only Ridge Œ±=1.0           | Overall OOF           | 0.4232 | ESM-1v beats ESM2              |
| VH+VL + TAP + Subtypes (no PCA)         | Overall OOF           | 0.4152 | TAP features hurt              |
| MLP (256‚Üí64) VH+VL                      | Overall OOF           | 0.0610 | Neural net failed              |
| ElasticNet VH+VL                        | Overall OOF           | -0.309 | L1 too harsh, predicted constants |

---

## üîç Key Findings

### ‚úÖ What Worked
1. **VH + VL Concatenation:** +22% improvement over VH-only (0.4646 ‚Üí 0.5676 OOF)
2. **ESM-1v over ESM2:** +9.8% improvement (0.4232 ‚Üí 0.4646)
3. **Alpha tuning:** Œ±=7.0 optimal for 2560D embeddings
4. **Ridge regression:** More stable than XGBoost, ElasticNet, or MLP

### ‚ùå What Didn't Work
1. **PCA dimensionality reduction:** 2560D ‚Üí 50D lost information
2. **TAP biophysical features:** Hurt performance for PR_CHO task
3. **Antibody subtype encoding:** No benefit for this task
4. **ElasticNet L1+L2:** Too aggressive, predicted constants
5. **Neural networks (MLP):** Severe overfitting on small dataset

### ü§î Remaining Challenges
1. **Fold 3 consistently struggles** (0.1224 Spearman) - difficult antibody subset?
2. **Gap to leaderboard leader:** 0.4722 vs 0.892 (89% higher!)
3. **Feature engineering:** Sequence embeddings alone may not be enough

---

## üÜö Benchmark Comparison

| Model                    | PR_CHO Spearman | Method                          |
|--------------------------|-----------------|--------------------------------|
| **Our Best Model**       | **0.4722**      | ESM-1v VH+VL Ridge Œ±=7.0       |
| piggen (benchmark #1)    | 0.424           | p-IgGen embeddings + Ridge     |
| esm2_ridge               | 0.420           | ESM2 embeddings + Ridge        |
| esm2_tap_ridge           | 0.413           | ESM2-PCA + TAP + Ridge         |
| ablang2_elastic_net      | 0.362           | AbLang2 paired + ElasticNet    |

**Result:** We beat all published benchmarks by 11%+ üéâ

---

## üìà Iteration Progress

**Starting point:** ESM2-650M VH-only = 0.4232
**Final score:** ESM-1v VH+VL Ridge Œ±=7.0 = 0.4722 (OOF: 0.5677)
**Total improvement:** +11.6% over baseline (OOF: +34.2%)

**Phases:**
1. ‚úÖ Model selection: ESM-1v > ESM2 (+9.8%)
2. ‚úÖ Feature engineering: VH+VL concatenation (+22% OOF)
3. ‚úÖ Hyperparameter tuning: Alpha sweep Œ±=7.0 (+0.01 OOF)
4. ‚ùå Additional features: PCA + TAP + Subtypes all hurt
5. ‚ùå Alternative regressors: XGBoost, ElasticNet, MLP all failed

---

## üéØ Leaderboard Context

**Current Leaderboard (as of submission):**
1. nagajyothi12344: **0.892**
2. boltzmann/ehottl: **0.885**
3. anon-quick-bee-2y: **0.750**
4. **Our submission:** **0.4722** (estimated)
5. Benchmark leader (piggen): **0.424**

**Gap analysis:** Top performers are 89% better than us. Possible reasons:
- Fine-tuned antibody-specific PLMs
- Ensemble of multiple models
- Structural features (AntiFold, DeepSP, Aggrescan3D)
- Heavy hyperparameter optimization
- Domain expertise in antibody engineering

---

## üöÄ Next Steps (If Pursuing Further)

**Quick wins (1-2 hours):**
1. Ensemble ESM-1v + ESM2 predictions (might get to 0.50)
2. Try different fold strategies (might improve Fold 3)
3. Feature selection on TAP (maybe some help, not all)

**Medium effort (1-2 days):**
4. Fine-tune ESM-1v on antibody data (SAbDab, OAS)
5. Try AbLang2 or p-IgGen antibody-specific PLMs
6. Implement weighted ensemble based on fold confidence

**High effort (1+ weeks):**
7. Integrate structural predictions (AntiBodyBuilder3)
8. Implement attention-based fusion of VH and VL (not just concat)
9. Use larger models (ESM2-3B, ESM2-15B) if compute allows
10. Develop antibody-specific data augmentation

---

## üìù Code Quality

**All checks passed:**
- ‚úÖ Ruff lint: All checks passed
- ‚úÖ Ruff format: 1 file reformatted
- ‚úÖ Mypy typecheck: Success (all type errors fixed)

**Files created:**
- `scripts/train_ginkgo_xgboost.py` - XGBoost regressor
- `scripts/train_ginkgo_vh_vl_concat.py` - VH+VL baseline (best model)
- `scripts/train_ginkgo_full_features.py` - PCA + TAP + Subtypes
- `scripts/train_ginkgo_no_pca.py` - TAP + Subtypes without PCA
- `scripts/train_ginkgo_elasticnet.py` - ElasticNet regressor
- `scripts/train_ginkgo_mlp.py` - Neural network regressor
- `scripts/alpha_sweep_vh_vl.py` - Alpha hyperparameter sweep
- `scripts/verify_spearman_calculation.py` - Metric verification
- `parse_alpha_sweep.py` - Results parser

---

## üèÅ Conclusion

**Achievement:** Beat all published benchmarks by 11%+ with simple VH+VL concatenation and alpha tuning.

**Limitation:** Still 89% gap to leaderboard leaders - suggests they're using advanced techniques beyond sequence embeddings alone.

**Recommendation:** For production antibody screening, our model (0.4722 Spearman) is solid and computationally efficient. For competition placement, significant additional work needed.

**Best use case:** Internal antibody library screening where 0.47 correlation is sufficient for prioritization, not competitive leaderboard placement.
