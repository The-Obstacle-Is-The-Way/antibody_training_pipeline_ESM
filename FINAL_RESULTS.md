# Ginkgo 2025 Competition - Final Results

**Date:** 2025-11-13
**Status:** Complete iteration, ready for your decision

---

## ðŸ† **BEST MODEL**

**ESM-1v VH+VL Ridge Î±=7.0**
- **Leaderboard Score:** 0.4722 (mean of per-fold Spearmans)
- **Overall OOF Spearman:** 0.5677
- **Model:** facebook/esm1v_t33_650M_UR90S_1
- **Features:** VH + VL concatenated embeddings (2560D)
- **Regressor:** Ridge regression with Î±=7.0

---

## ðŸ“Š **ALL EXPERIMENTS FINAL RESULTS**

| Experiment                      | Mean per-fold | Overall OOF | vs Baseline | vs Benchmark |
|---------------------------------|---------------|-------------|-------------|--------------|
| **ESM-1v VH+VL Ridge Î±=7** âœ…   | **0.4722**    | **0.5677**  | +34.2%      | **+11.3%**   |
| p-IgGen VH+VL Ridge Î±=7         | 0.4171        | 0.4939      | +18.6%      | -1.7%        |
| ESM-1v VH+VL Ridge Î±=5          | 0.4682        | 0.5676      | +34.1%      | +10.4%       |
| ESM-1v VH-only Ridge Î±=5        | 0.4382        | 0.4937      | +16.7%      | +3.4%        |
| XGBoost VH-only                 | â€”             | 0.4369      | +3.2%       | +3.1%        |
| ESM2-650M VH-only Ridge Î±=1     | â€”             | 0.4232      | **Baseline** | â€”           |

**Benchmark comparison:** piggen (official benchmark) = 0.424 mean per-fold Spearman

---

## âœ… **WHAT WORKED**

1. **VH + VL Concatenation** â†’ +22% OOF improvement (0.4646 â†’ 0.5676)
   - Captures full antibody structure instead of just heavy chain
   - Single biggest win!

2. **ESM-1v over ESM2** â†’ +9.8% improvement (0.4232 â†’ 0.4646)
   - Variant-specialized model better for antibody sequences

3. **Alpha tuning** â†’ +0.01 OOF improvement (Î±=5.0 â†’ Î±=7.0)
   - Light regularization optimal for high-dimensional embeddings

4. **Ridge over alternatives** â†’ Ridge most stable
   - XGBoost: inconsistent across folds
   - ElasticNet: too aggressive, predicted constants
   - MLP: severe overfitting

---

## âŒ **WHAT DIDN'T WORK**

1. **PCA dimensionality reduction** (2560D â†’ 50D)
   - Lost too much information
   - Score dropped from 0.5676 â†’ 0.5388

2. **TAP biophysical features**
   - SFvCSP, PSH, PPC, PNC, CDR Length
   - Hurt PR_CHO prediction (dropped to 0.4152)
   - May help other assays (HIC, Tm2) but not polyreactivity

3. **Antibody subtype encoding** (IgG1/2/4, Kappa/Lambda)
   - No benefit for PR_CHO
   - Important for Tm2 (thermal stability) but not polyreactivity

4. **p-IgGen antibody-specific PLM**
   - Despite being trained on antibody data
   - Still 13% worse than ESM-1v VH+VL (0.4171 vs 0.4722)
   - Smaller model (768D vs 2560D) may lack capacity

5. **Neural networks (MLP)**
   - Severe overfitting on 197 samples
   - Score: 0.0610 (vs 0.4722 with Ridge)

---

## ðŸ†š **BENCHMARK COMPARISON**

**Official Benchmark (from abdev-benchmark repo):**

| Model                 | PR_CHO Spearman | Method                     |
|-----------------------|-----------------|----------------------------|
| **Our Model**         | **0.4722**      | ESM-1v VH+VL Ridge Î±=7     |
| piggen                | 0.424           | p-IgGen + Ridge            |
| esm2_ridge            | 0.420           | ESM2 + Ridge               |
| esm2_tap_ridge        | 0.413           | ESM2-PCA + TAP + Ridge     |
| ablang2_elastic_net   | 0.362           | AbLang2 paired + ElasticNet |

**Result:** We beat all published benchmarks by 11%+ ðŸŽ‰

---

## ðŸŽ¯ **LEADERBOARD CONTEXT**

**Current Leaderboard:**
1. nagajyothi12344: **0.892** ðŸ”¥
2. boltzmann/ehottl: **0.885**
3. anon-quick-bee-2y: **0.750**
4. **Our submission:** **0.4722** (estimated)
5. Benchmark leader: **0.424**

**Gap to #1:** 0.892 - 0.4722 = **0.420 gap** (89% higher)

**Likely reasons for gap:**
1. Ensemble of multiple PLMs (ESM-1v + ESM2 + AbLang2 + p-IgGen)
2. Fine-tuned models on antibody-specific data (SAbDab, OAS)
3. Structural features (AntiFold, DeepSP, Aggrescan3D)
4. Advanced feature engineering (CDR properties, charge, hydrophobicity)
5. Heavy hyperparameter optimization (alpha, model architecture)
6. Domain expertise in antibody engineering

---

## ðŸ¤” **FOLD 3 PROBLEM**

Fold 3 consistently struggles across all experiments:

| Model                | Fold 0 | Fold 1 | Fold 2 | Fold 3  | Fold 4 |
|----------------------|--------|--------|--------|---------|--------|
| ESM-1v VH+VL Î±=7     | 0.4571 | 0.5459 | 0.6371 | **0.1224** | 0.5983 |
| p-IgGen Î±=7          | 0.4324 | 0.4663 | 0.5353 | **0.3247** | 0.3266 |

Fold 3 is either:
- Inherently difficult antibody subset
- Different distribution from other folds
- Needs different features/model

---

## ðŸ“ **FILES CREATED**

**Training Scripts:**
- `scripts/train_ginkgo_xgboost.py` - XGBoost regressor
- `scripts/train_ginkgo_vh_vl_concat.py` - **VH+VL baseline (BEST)**
- `scripts/train_ginkgo_full_features.py` - PCA + TAP + Subtypes
- `scripts/train_ginkgo_no_pca.py` - TAP + Subtypes without PCA
- `scripts/train_ginkgo_elasticnet.py` - ElasticNet regressor
- `scripts/train_ginkgo_mlp.py` - Neural network
- `scripts/train_ginkgo_piggen.py` - p-IgGen antibody PLM
- `scripts/alpha_sweep_vh_vl.py` - Hyperparameter sweep
- `scripts/verify_spearman_calculation.py` - Metric verification

**Utilities:**
- `parse_alpha_sweep.py` - Parse Hydra multirun results

**Documentation:**
- `GINKGO_RESULTS_SUMMARY.md` - Full results breakdown
- `FINAL_RESULTS.md` - This file

---

## âœ… **CODE QUALITY**

- âœ… **Ruff lint:** All experimental scripts linted
- âœ… **Ruff format:** Auto-formatted
- âœ… **Mypy typecheck:** Core code passes (scripts have minor import order warnings)

---

## ðŸš€ **WHAT'S NEXT? (YOUR DECISION)**

### Option 1: **Ship it now (0.4722)**
- Beat all published benchmarks by 11%+
- Solid baseline for internal use
- Won't place competitively (likely bottom 30%)

### Option 2: **Quick ensemble (1-2 hours)**
- Average ESM-1v + p-IgGen predictions
- Might get to 0.48-0.50
- Still won't crack top 3

### Option 3: **Heavy optimization (1+ weeks)**
- Fine-tune ESM-1v on antibody data
- Ensemble multiple PLMs
- Integrate structural features
- Might reach 0.60-0.70 (still not 0.89)

### Option 4: **Move on**
- Competition isn't our core use case
- 0.47 correlation is good for internal screening
- Focus on your actual antibody pipeline

---

## ðŸ’¡ **MY RECOMMENDATION**

**Don't chase 0.89.** Here's why:

1. **You're already winning where it matters:** 0.4722 beats all benchmarks
2. **Diminishing returns:** Getting from 0.47 â†’ 0.89 requires massive effort
3. **Use case mismatch:** Leaderboard optimizes for competition, not production utility
4. **Your real goal:** Screen antibody libraries with decent correlation - YOU HAVE THAT

**Ship 0.4722 as a baseline** and move on to your actual antibody engineering work. Come back to this if you need to optimize further.

---

## ðŸ“ˆ **PROGRESS TIMELINE**

- **Starting point:** ESM2-650M VH-only = 0.4232
- **After model selection:** ESM-1v VH-only = 0.4646 (+9.8%)
- **After feature engineering:** ESM-1v VH+VL = 0.5676 OOF (+34.2%)
- **After hyperparameter tuning:** ESM-1v VH+VL Î±=7 = 0.4722 mean per-fold (BEST)
- **After trying alternatives:** All failed to beat baseline

**Total improvement:** +11.6% over baseline (mean per-fold metric)

---

## ðŸŽ“ **KEY LEARNINGS**

1. **VH+VL concatenation >>> VH-only** for antibody tasks
2. **ESM-1v >> ESM2** for variant prediction
3. **Simple Ridge >> complex models** on small datasets (197 samples)
4. **Sequence embeddings alone >> hand-crafted features** for PR_CHO
5. **Antibody-specific PLMs don't always win** (p-IgGen < ESM-1v)

---

## ðŸ **FINAL VERDICT**

**Achievement:** âœ… Beat all published benchmarks by 11%+
**Leaderboard placement:** âŒ Still 89% gap to #1
**Production readiness:** âœ… 0.47 correlation good for screening
**Competition viability:** âŒ Not worth chasing 0.89

**Recommendation:** Ship it or move on. You've won the benchmark battle. ðŸŽ¯
