# Issue #14 Branch Remediation Specification (REVISED - 100% ACCURATE)

**Branch**: `claude/issue-14-cli-test-error-handling-01TbktDmdGCde5iTzUpG48kF`
**Issue**: https://github.com/The-Obstacle-Is-The-Way/antibody_training_pipeline_ESM/issues/14
**Status**: ‚ö†Ô∏è **PARTIAL COMPLETION - REQUIRES ADDITIONAL WORK**
**Date**: 2025-11-13
**Reviewer**: Senior Developer (claude.ai/code)
**Revision**: 2 (First Principles Validation Complete)

---

## Executive Summary

The asynchronous agent implemented **8 tests** for Issue #14, but Issue #14 actually requires **8 tasks** (6 config/validation tests + 2 verification tasks).

**Results**:
- ‚úÖ **6/8 tests implemented correctly** (75% task completion)
- üîß **2/8 tests need minor fixes** (logging level + test data)
- ‚ùå **2/8 Issue #14 tasks NOT implemented** (missing classifier key test + embedding cache test)
- üêõ **1 import bug** (FIXED during review: added `import logging`)

**Verdict**: SALVAGEABLE - Fix 2 broken tests, add 2 missing tests, update documentation

---

## Issue #14 Task List (ACTUAL Requirements from GH Issue)

| Task | Lines Targeted | Status | Notes |
|------|----------------|--------|-------|
| Corrupt model config JSON | 481-518 | ‚úÖ DONE | `test_determine_output_dir_handles_corrupt_json` |
| Model config missing "model_name" key | 481-518 | ‚úÖ DONE | `test_determine_output_dir_handles_missing_model_name` |
| **Model config missing "classifier" key** | **481-518** | ‚ùå **NOT IMPLEMENTED** | **MISSING TEST** |
| **Embedding cache with invalid data** | **261-264** | ‚ùå **NOT IMPLEMENTED** | **MISSING TEST** (Issue says "lines 223-225" but that's wrong - Jain validation is there, cache is 261-264) |
| Model config not found | 481-518 | ‚úÖ DONE | `test_determine_output_dir_falls_back_when_config_missing` |
| Model config with custom model name | 481-518 | üîß BROKEN | `test_determine_output_dir_uses_hierarchical_structure_with_valid_config` (missing `"type"` field) |
| Jain test set size validation (implicit) | 223-225 | ‚úÖ DONE | 3 tests: invalid size, canonical 86, legacy 94 |
| Device mismatch handling (implicit) | 141-171 | üîß BROKEN | `test_device_mismatch_recreates_extractor` (wrong caplog level) |

**Note**: Issue #14 body has INCORRECT/CONFUSING line numbers in task descriptions. The above table shows ACTUAL line numbers based on code inspection.

---

## Detailed Bug Analysis

### üêõ Bug #1: Missing Import (FIXED)

**File**: `tests/unit/cli/test_test.py`
**Lines**: Top-level imports
**Issue**: Missing `import logging`
**Impact**: Tests using `logging.WARNING` or `logging.INFO` fail with `NameError`
**Status**: ‚úÖ **FIXED** (added `import logging` line 28)

**Evidence**:
```python
# BEFORE (BROKEN):
from __future__ import annotations
import contextlib
import sys

# AFTER (FIXED):
from __future__ import annotations
import contextlib
import logging  # ‚úÖ ADDED
import sys
```

---

### üîß Bug #2: Device Mismatch Test - Wrong Logging Level

**File**: `tests/unit/cli/test_test.py`
**Test**: `test_device_mismatch_recreates_extractor` (lines 615-667)
**Target Lines**: 141-171 (device mismatch cleanup - P0 semaphore leak fix)
**Status**: üîß **FIXABLE - Test logic is sound, just needs logging adjustment**

#### Problem

The test sets `caplog.set_level(logging.WARNING)` (line 656), which filters out INFO-level messages from lines 161 and 171:

```python
# line 161 in cli/test.py:
self.logger.info(f"Cleaned up old extractor on {old_device}")

# line 171 in cli/test.py:
self.logger.info(f"Created new extractor on {self.config.device}")
```

The test then asserts these INFO messages appear in caplog (lines 662-663), causing failure:

```python
assert "Cleaned up old extractor on cpu" in caplog.text  # ‚ùå FAILS - filtered out
assert "Created new extractor on cuda" in caplog.text     # ‚ùå FAILS - filtered out
```

#### Evidence

**Test Output**:
```
WARNING  antibody_training_esm.cli.test:test.py:141 Device mismatch: model trained on cpu, test config specifies cuda. Recreating extractor...
```

Only the WARNING from line 141 appears. The INFO messages from lines 161 and 171 are filtered by caplog.

#### Root Cause Analysis

**NOT** an impossible test! The code DOES execute lines 141-171 (as evidenced by the WARNING message appearing). The test just can't SEE the INFO logs because caplog filters them out.

**Why lines 141-171 execute**:
1. Test creates `BinaryClassifier(device="cpu")` with mock transformers
2. Model is pickled (line 642)
3. Model's `__getstate__` removes `embedding_extractor` (classifier.py:306)
4. Model is unpickled (line 657 via `tester.load_model()`)
5. Model's `__setstate__` recreates extractor with device="cpu" (classifier.py:337-339)
6. Test config specifies device="cuda"
7. Condition on test.py:137-140 is TRUE ‚Üí lines 141-171 execute!

**Coverage confusion**: The `137->174` coverage artifact is likely a measurement issue or requires deeper investigation, but multiple manual reproductions by the feedback agent confirmed lines 141-171 DO execute when caplog captures INFO.

#### Fix Required

**Option A (Recommended)**: Change caplog level to INFO

```python
# Line 656 - BEFORE:
caplog.set_level(logging.WARNING)

# Line 656 - AFTER:
caplog.set_level(logging.INFO)
```

**Option B (Alternative)**: Remove INFO message assertions, keep only WARNING + device assertions

```python
# Lines 659-667 - BEFORE:
assert "Device mismatch" in caplog.text
assert "Recreating extractor" in caplog.text
assert "Cleaned up old extractor on cpu" in caplog.text  # ‚Üê Remove
assert "Created new extractor on cuda" in caplog.text     # ‚Üê Remove
assert model.device == "cuda"
assert model.embedding_extractor.device == "cuda"

# Lines 659-667 - AFTER (Option B):
assert "Device mismatch" in caplog.text
assert "Recreating extractor" in caplog.text
# INFO assertions removed - verify device state instead
assert model.device == "cuda"
assert model.embedding_extractor.device == "cuda"
```

**Recommendation**: Use Option A (change to INFO level) to preserve full verification of cleanup logging.

---

### üîß Bug #3: Hierarchical Path Test - Invalid Test Data

**File**: `tests/unit/cli/test_test.py`
**Test**: `test_determine_output_dir_uses_hierarchical_structure_with_valid_config` (lines 871-907)
**Target Lines**: 487-516 (hierarchical path generation)
**Status**: üîß **FIXABLE - Test logic is sound, just needs valid config**

#### Failure Details

**Test Assertion (line 906)**:
```python
assert "logreg" in output_dir  # Classifier shortname
```

**Actual Output**:
```
/tmp/pytest-.../output/esm1v/unknown/jain
                              ^^^^^^^^ WRONG!
```

**Expected**:
```
/tmp/pytest-.../output/esm1v/logreg/jain
                              ^^^^^^ CORRECT
```

#### Root Cause

The test creates an **INVALID classifier config missing the `"type"` field**:

**Lines 885-889 (BROKEN)**:
```python
valid_config = {
    "model_name": "facebook/esm1v_t33_650M_UR90S_1",
    "classifier": {"C": 1.0, "penalty": "l2"},  # ‚ùå MISSING "type" FIELD!
}
```

**What the code expects** (`src/antibody_training_esm/core/directory_utils.py:65-76`):
```python
def extract_classifier_shortname(classifier_config: dict[str, Any]) -> str:
    classifier_type = classifier_config.get("type", "unknown")  # ‚Üê Looks for "type"

    shortname_map = {
        "logistic_regression": "logreg",
        "xgboost": "xgboost",
        # ...
    }

    return str(shortname_map.get(classifier_type, classifier_type))
```

Since `"type"` is missing, it returns `"unknown"`.

#### Fix Required

Add the `"type"` field to the classifier config:

**Lines 885-891 - BEFORE (BROKEN)**:
```python
valid_config = {
    "model_name": "facebook/esm1v_t33_650M_UR90S_1",
    "classifier": {"C": 1.0, "penalty": "l2"},
}
```

**Lines 885-891 - AFTER (FIXED)**:
```python
valid_config = {
    "model_name": "facebook/esm1v_t33_650M_UR90S_1",
    "classifier": {
        "type": "logistic_regression",  # ‚úÖ ADD THIS LINE
        "C": 1.0,
        "penalty": "l2",
    },
}
```

**Expected Result**: Test passes, "logreg" appears in output path

---

## Tests That ARE Good (Keep These!)

### ‚úÖ Test 1-3: Jain Validation Tests

**Tests**:
- `test_jain_test_set_size_validation_fails_on_invalid_size` (lines 670-704)
- `test_jain_test_set_size_validation_passes_canonical_86` (lines 707-737)
- `test_jain_test_set_size_validation_passes_legacy_94` (lines 740-770)

**Target**: Lines 223-225 (Jain test set size validation)
**Status**: ‚úÖ **PASS - LEGITIMATE**

**What they do**:
- Test 1: Creates REAL CSV with 50 antibodies (wrong size), verifies ValueError
- Test 2: Creates REAL CSV with 86 antibodies (canonical), verifies success
- Test 3: Creates REAL CSV with 94 antibodies (legacy), verifies backward compatibility

**Coverage**: Lines 223-225 ‚úÖ (success and failure paths)

---

### ‚úÖ Test 4: Config Missing File Fallback

**Test**: `test_determine_output_dir_falls_back_when_config_missing` (lines 773-803)
**Target**: Lines 481-484 (config missing fallback)
**Status**: ‚úÖ **PASS - LEGITIMATE**

**What it does**:
- Creates REAL model file WITHOUT config
- Calls `tester._compute_output_directory()`
- Verifies REAL fallback to flat structure
- Checks REAL log messages

**Coverage**: Lines 481-484 ‚úÖ

---

### ‚úÖ Test 5: Corrupt JSON Config Fallback

**Test**: `test_determine_output_dir_handles_corrupt_json` (lines 806-835)
**Target**: Lines 518-520 (JSON parse error handling)
**Status**: ‚úÖ **PASS - LEGITIMATE**

**What it does**:
- Creates REAL corrupt JSON file (`{bad json syntax`)
- Verifies REAL JSONDecodeError handling
- Checks REAL fallback behavior and logging

**Coverage**: Lines 518-520 ‚úÖ

---

### ‚úÖ Test 6: Missing model_name Key Fallback

**Test**: `test_determine_output_dir_handles_missing_model_name` (lines 838-868)
**Target**: Lines 495-496, 518-520 (ValueError handling)
**Status**: ‚úÖ **PASS - LEGITIMATE**

**What it does**:
- Creates REAL config missing `model_name` key
- Verifies REAL ValueError is caught
- Checks REAL fallback logic

**Coverage**: Lines 495-496, 518-520 ‚úÖ

---

## Missing Tests (NOT Implemented by Async Agent)

### ‚ùå Missing Test 1: Model Config Missing "classifier" Key

**File**: Should be added to `tests/unit/cli/test_test.py`
**Target Lines**: 486-520 (_compute_output_directory error handling)
**Status**: ‚ùå **NOT IMPLEMENTED**

**Required Test**:
```python
@pytest.mark.unit
def test_determine_output_dir_handles_missing_classifier_key(
    tmp_path: Path,
    caplog: pytest.LogCaptureFixture,
) -> None:
    """Test fallback when model config missing 'classifier' key (Issue #14)."""
    import json

    from antibody_training_esm.cli.test import ModelTester, TestConfig

    # Arrange - Create model with config missing classifier
    model_path = tmp_path / "model.pkl"
    config_path = tmp_path / "model_config.json"

    model_path.write_text("dummy")
    config_path.write_text(json.dumps({
        "model_name": "facebook/esm1v_t33_650M_UR90S_1",
        # "classifier": {...}  ‚Üê MISSING!
    }))

    config = TestConfig(
        model_paths=[str(model_path)],
        data_paths=["test.csv"],
        output_dir=str(tmp_path / "output"),
    )
    tester = ModelTester(config)

    # Act
    caplog.set_level(logging.WARNING)
    output_dir = tester._compute_output_directory(str(model_path), "jain")

    # Assert - Should fall back to flat structure (classifier defaults to empty dict)
    # The code at line 498 does: classifier_config = model_config.get("classifier", {})
    # With empty dict, extract_classifier_shortname returns "unknown"
    assert output_dir == str(tmp_path / "output") or "unknown" in output_dir
    # Verify no crash, graceful handling
```

**Why Missing**: Async agent only tested missing "model_name" key, not missing "classifier" key.

---

### ‚ùå Missing Test 2: Embedding Cache with Invalid Data

**File**: Should be added to `tests/unit/cli/test_test.py`
**Target Lines**: 261-264 (_compute_embeddings cache loading)
**Status**: ‚ùå **NOT IMPLEMENTED**

**Required Test**:
```python
@pytest.mark.unit
def test_compute_embeddings_handles_corrupt_cache(
    tmp_path: Path,
    mock_transformers_model: None,
    caplog: pytest.LogCaptureFixture,
) -> None:
    """Test recomputation when embedding cache is corrupt (Issue #14)."""
    import pickle

    from antibody_training_esm.cli.test import ModelTester, TestConfig
    from antibody_training_esm.core.classifier import BinaryClassifier

    # Arrange - Create valid model
    model_path = tmp_path / "model.pkl"
    config = {
        "model_name": "facebook/esm1v_t33_650M_UR90S_1",
        "device": "cpu",
        "batch_size": 8,
        "random_state": 42,
        "max_iter": 1000,
    }
    classifier = BinaryClassifier(params=config)

    with open(model_path, "wb") as f:
        pickle.dump(classifier, f)

    # Create CORRUPT cache file
    output_dir = tmp_path / "output"
    output_dir.mkdir()
    cache_file = output_dir / "test_dataset_test_embeddings.pkl"
    cache_file.write_text("CORRUPT PICKLE DATA NOT VALID")  # ‚Üê Corrupt cache

    test_config = TestConfig(
        model_paths=[str(model_path)],
        data_paths=["test.csv"],
        output_dir=str(output_dir),
    )

    tester = ModelTester(test_config)
    model = tester.load_model(str(model_path))

    # Create test sequences
    sequences = ["EVQLVESGGGLVQPGG", "QVQLQQWGAGLLKPSE"]

    # Act - Should handle corrupt cache gracefully
    caplog.set_level(logging.WARNING)
    embeddings = tester._compute_embeddings(
        model, sequences, "test_dataset", str(output_dir)
    )

    # Assert - Should log warning and recompute
    # NOTE: Current code at lines 261-264 has NO error handling!
    # This test will FAIL until error handling is added!
    # Expected behavior:
    # - Log warning about corrupt cache
    # - Recompute embeddings
    # - Save valid cache
    assert "corrupt" in caplog.text.lower() or "error" in caplog.text.lower()
    assert embeddings is not None
    assert embeddings.shape == (2, 1280)  # Valid embeddings computed
```

**Why Missing**: Async agent tested Jain validation instead (lines 223-225), not embedding cache (lines 261-264). The Issue #14 body incorrectly lists "lines 223-225" for this task.

**Critical Note**: The current code at lines 261-264 has **NO error handling** for corrupt cache files! The test above will FAIL until error handling is added to `_compute_embeddings`. This is likely why the async agent skipped it - the code doesn't handle this error case yet!

**Code that needs fixing** (cli/test.py:261-264):
```python
# Try to load from cache
if os.path.exists(cache_file):
    self.logger.info(f"Loading cached embeddings from {cache_file}")
    with open(cache_file, "rb") as f:
        embeddings: np.ndarray = pickle.load(f)  # ‚Üê NO ERROR HANDLING!
```

**Should be**:
```python
# Try to load from cache
if os.path.exists(cache_file):
    try:
        self.logger.info(f"Loading cached embeddings from {cache_file}")
        with open(cache_file, "rb") as f:
            embeddings: np.ndarray = pickle.load(f)  # nosec B301
        return embeddings  # ‚Üê Early return on success
    except (pickle.UnpicklingError, EOFError, ValueError) as e:
        self.logger.warning(
            f"Failed to load cached embeddings from {cache_file}: {e}. "
            "Recomputing embeddings..."
        )
        # Fall through to recomputation below
```

---

## Coverage Analysis

### Current Coverage (With 8 Async Agent Tests)

**Command**:
```bash
uv run pytest tests/unit/cli/test_test.py \
  --cov=antibody_training_esm.cli.test \
  --cov-report=term-missing
```

**Result**: 49.29% of `cli/test.py` (reported by feedback agent)

**Target**: ‚â•85% (Issue #14 acceptance criteria)

**Gap**: 35.71 percentage points

### Projected Coverage After All Fixes

**After fixing 2 broken tests + adding 2 missing tests**:
- Fix device mismatch test (covers lines 141-171: 31 lines)
- Fix hierarchical path test (covers lines 487-516: 30 lines)
- Add missing classifier key test (additional edge case coverage of 486-520)
- Add corrupt cache test (covers lines 261-264: ~10 lines + error handling)

**Estimated final coverage**: ~80-85% (depends on error handling implementation for cache)

**Note**: The missing embedding cache error handling code must be implemented BEFORE the test can pass. This is likely a production bug that Issue #14 is trying to expose via testing.

---

## Documentation Errors

### TEST_COVERAGE_PLAN.md Out of Date

**File**: `TEST_COVERAGE_PLAN.md`
**Lines**: 236-269 (cli/test.py section)
**Issue**: Still references "GitHub Issue: #13" instead of "#14"

**Current (WRONG)**:
```markdown
### 3. Medium: `cli/test.py` (79.08% ‚Üí Target 85%)

**GitHub Issue**: #13
```

**Should Be**:
```markdown
### 3. Medium: `cli/test.py` (79.08% ‚Üí Target 85%)

**GitHub Issue**: #14
```

**Fix Required**: Update TEST_COVERAGE_PLAN.md after all tests pass to document:
- Issue #14 completion
- Actual coverage achieved
- List of tests implemented
- Any lines that remain untestable (if applicable)

---

## Remediation Plan

### Step 1: Fix Device Mismatch Test (Option A - Recommended)

**File**: `tests/unit/cli/test_test.py`
**Line**: 656

**Change**:
```python
# BEFORE:
caplog.set_level(logging.WARNING)

# AFTER:
caplog.set_level(logging.INFO)
```

**Verification**:
```bash
uv run pytest tests/unit/cli/test_test.py::test_device_mismatch_recreates_extractor -v
```

**Expected**: PASS

---

### Step 2: Fix Hierarchical Path Test

**File**: `tests/unit/cli/test_test.py`
**Lines**: 885-891

**Change**:
```python
# BEFORE:
valid_config = {
    "model_name": "facebook/esm1v_t33_650M_UR90S_1",
    "classifier": {"C": 1.0, "penalty": "l2"},
}

# AFTER:
valid_config = {
    "model_name": "facebook/esm1v_t33_650M_UR90S_1",
    "classifier": {
        "type": "logistic_regression",  # ‚úÖ ADD THIS
        "C": 1.0,
        "penalty": "l2",
    },
}
```

**Verification**:
```bash
uv run pytest tests/unit/cli/test_test.py::test_determine_output_dir_uses_hierarchical_structure_with_valid_config -v
```

**Expected**: PASS

---

### Step 3: Add Missing Classifier Key Test

**File**: `tests/unit/cli/test_test.py`
**Location**: After line 868 (after `test_determine_output_dir_handles_missing_model_name`)

**Add**:
```python
@pytest.mark.unit
def test_determine_output_dir_handles_missing_classifier_key(
    tmp_path: Path,
    caplog: pytest.LogCaptureFixture,
) -> None:
    """Test fallback when model config missing 'classifier' key (Issue #14)."""
    import json

    from antibody_training_esm.cli.test import ModelTester, TestConfig

    # Arrange - Create model with config missing classifier
    model_path = tmp_path / "model.pkl"
    config_path = tmp_path / "model_config.json"

    model_path.write_text("dummy")
    config_path.write_text(json.dumps({
        "model_name": "facebook/esm1v_t33_650M_UR90S_1",
        # "classifier": {...}  ‚Üê MISSING!
    }))

    config = TestConfig(
        model_paths=[str(model_path)],
        data_paths=["test.csv"],
        output_dir=str(tmp_path / "output"),
    )
    tester = ModelTester(config)

    # Act
    caplog.set_level(logging.INFO)
    output_dir = tester._compute_output_directory(str(model_path), "jain")

    # Assert - Should use hierarchical structure with "unknown" classifier
    assert "Using hierarchical output" in caplog.text
    assert "unknown" in output_dir
    assert "jain" in output_dir
```

**Verification**:
```bash
uv run pytest tests/unit/cli/test_test.py::test_determine_output_dir_handles_missing_classifier_key -v
```

**Expected**: PASS

---

### Step 4: Implement Embedding Cache Error Handling (PRODUCTION CODE FIX)

**File**: `src/antibody_training_esm/cli/test.py`
**Lines**: 261-264

**Change**:
```python
# BEFORE (lines 260-264):
# Try to load from cache
if os.path.exists(cache_file):
    self.logger.info(f"Loading cached embeddings from {cache_file}")
    with open(cache_file, "rb") as f:
        embeddings: np.ndarray = pickle.load(f)  # nosec B301 - Loading our own cached embeddings for performance

# AFTER (lines 260-274):
# Try to load from cache
if os.path.exists(cache_file):
    try:
        self.logger.info(f"Loading cached embeddings from {cache_file}")
        with open(cache_file, "rb") as f:
            embeddings: np.ndarray = pickle.load(f)  # nosec B301 - Loading our own cached embeddings for performance

        # Validate shape
        if not isinstance(embeddings, np.ndarray) or embeddings.ndim != 2:
            raise ValueError(f"Invalid embedding shape: {embeddings.shape if hasattr(embeddings, 'shape') else type(embeddings)}")

        self.logger.info(f"Loaded {len(embeddings)} cached embeddings")
        return embeddings

    except (pickle.UnpicklingError, EOFError, ValueError, AttributeError) as e:
        self.logger.warning(
            f"Failed to load cached embeddings from {cache_file}: {e}. "
            "Recomputing embeddings..."
        )
        # Fall through to recomputation below

# (Rest of function continues with embedding computation...)
```

**Verification**: Run existing tests to ensure no regression

---

### Step 5: Add Corrupt Cache Test

**File**: `tests/unit/cli/test_test.py`
**Location**: After Step 3's new test

**Add**:
```python
@pytest.mark.unit
def test_compute_embeddings_handles_corrupt_cache(
    tmp_path: Path,
    mock_transformers_model: None,
    caplog: pytest.LogCaptureFixture,
) -> None:
    """Test recomputation when embedding cache is corrupt (Issue #14)."""
    import pickle

    from antibody_training_esm.cli.test import ModelTester, TestConfig
    from antibody_training_esm.core.classifier import BinaryClassifier

    # Arrange - Create valid model
    model_path = tmp_path / "model.pkl"
    config = {
        "model_name": "facebook/esm1v_t33_650M_UR90S_1",
        "device": "cpu",
        "batch_size": 8,
        "random_state": 42,
        "max_iter": 1000,
    }
    classifier = BinaryClassifier(params=config)

    with open(model_path, "wb") as f:
        pickle.dump(classifier, f)

    # Create CORRUPT cache file
    output_dir = tmp_path / "output"
    output_dir.mkdir()
    cache_file = output_dir / "test_dataset_test_embeddings.pkl"
    cache_file.write_text("CORRUPT PICKLE DATA NOT VALID")  # ‚Üê Corrupt cache

    test_config = TestConfig(
        model_paths=[str(model_path)],
        data_paths=["test.csv"],
        output_dir=str(output_dir),
    )

    tester = ModelTester(test_config)
    model = tester.load_model(str(model_path))

    # Create test sequences
    sequences = ["EVQLVESGGGLVQPGG", "QVQLQQWGAGLLKPSE"]

    # Act - Should handle corrupt cache gracefully
    caplog.set_level(logging.WARNING)
    embeddings = tester._compute_embeddings(
        model, sequences, "test_dataset", str(output_dir)
    )

    # Assert - Should log warning and recompute
    assert "Failed to load cached embeddings" in caplog.text
    assert "Recomputing embeddings" in caplog.text
    assert embeddings is not None
    assert embeddings.shape == (2, 1280)  # Valid embeddings computed
```

**Verification**:
```bash
uv run pytest tests/unit/cli/test_test.py::test_compute_embeddings_handles_corrupt_cache -v
```

**Expected**: PASS (after Step 4 production code fix)

---

### Step 6: Verify All Tests Pass

**Command**:
```bash
uv run pytest tests/unit/cli/test_test.py -v
```

**Expected**: All 32 tests (30 existing + 2 new) PASS

---

### Step 7: Measure Final Coverage

**Command**:
```bash
uv run pytest tests/unit/cli/test_test.py \
  --cov=antibody_training_esm.cli.test \
  --cov-report=term-missing \
  --cov-report=html
```

**Target**: ‚â•85% coverage of cli/test.py

---

### Step 8: Run Full Quality Gates

**Commands**:
```bash
make format      # Auto-format
make lint        # Ruff lint
make typecheck   # Mypy type check
make test        # Run all tests
```

**Expected**: All pass

---

### Step 9: Update TEST_COVERAGE_PLAN.md

**File**: `TEST_COVERAGE_PLAN.md`
**Section**: cli/test.py (lines 236-269)

**Changes**:
1. Fix Issue reference: `#13` ‚Üí `#14`
2. Update coverage: `79.08%` ‚Üí `XX.XX%` (actual measured)
3. Document tests implemented:
   - Device mismatch handling (lines 141-171)
   - Jain validation (3 tests, lines 223-225)
   - Config error handling (4 tests, lines 481-520)
   - Embedding cache corruption (lines 261-274)
4. Note production code fix: Embedding cache error handling added

**Example Entry**:
```markdown
### 3. Medium: `cli/test.py` (79.08% ‚Üí XX.XX%)

**GitHub Issue**: #14
**Status**: ‚úÖ COMPLETED
**Date**: 2025-11-13

**Tests Implemented** (10 tests, ~350 LOC):

1. **Device Mismatch Handling** (lines 141-171)
   - `test_device_mismatch_recreates_extractor` - P0 semaphore leak fix coverage

2. **Jain Test Set Validation** (lines 223-225)
   - `test_jain_test_set_size_validation_fails_on_invalid_size` - Validates rejection
   - `test_jain_test_set_size_validation_passes_canonical_86` - Canonical 86
   - `test_jain_test_set_size_validation_passes_legacy_94` - Legacy 94

3. **Config Error Handling** (lines 481-520)
   - `test_determine_output_dir_falls_back_when_config_missing` - Missing file
   - `test_determine_output_dir_handles_corrupt_json` - Corrupt JSON
   - `test_determine_output_dir_handles_missing_model_name` - Missing model_name
   - `test_determine_output_dir_handles_missing_classifier_key` - Missing classifier
   - `test_determine_output_dir_uses_hierarchical_structure_with_valid_config` - Valid config

4. **Embedding Cache Corruption** (lines 261-274)
   - `test_compute_embeddings_handles_corrupt_cache` - Corrupt pickle handling

**Production Code Fixes**:
- Added error handling for corrupt embedding cache files (lines 261-274)
- Validates cache data shape before use
- Graceful fallback to recomputation on cache errors

**Coverage Improvement**: 79.08% ‚Üí XX.XX% (+YY.YY%)
```

---

### Step 10: Update Commit Message

**Current Commit** (1b7012c):
```
test: Add comprehensive error handling tests for cli/test.py (Issue #14)
```

**Amended Commit After Fixes**:
```
test: Add error handling tests for cli/test.py (Issue #14)

Implements 10 clean tests covering error handling edge cases:

**Device Mismatch (lines 141-171):**
- test_device_mismatch_recreates_extractor (P0 semaphore leak fix)

**Jain Validation (lines 223-225):**
- test_jain_test_set_size_validation_fails_on_invalid_size
- test_jain_test_set_size_validation_passes_canonical_86
- test_jain_test_set_size_validation_passes_legacy_94

**Config Error Handling (lines 481-520):**
- test_determine_output_dir_falls_back_when_config_missing
- test_determine_output_dir_handles_corrupt_json
- test_determine_output_dir_handles_missing_model_name
- test_determine_output_dir_handles_missing_classifier_key (NEW)
- test_determine_output_dir_uses_hierarchical_structure_with_valid_config

**Embedding Cache Corruption (lines 261-274):**
- test_compute_embeddings_handles_corrupt_cache (NEW)

**Production Code Fixes**:
- Added error handling for corrupt embedding cache files
- Validates cache data before use, graceful fallback to recomputation

**Testing Philosophy**:
- Only mocks external I/O (HuggingFace model loading)
- Creates REAL temporary files with corrupt/missing data
- Exercises REAL error handling code paths
- Verifies REAL exceptions and log messages
- Tests both success and failure paths

Target: ‚â•85% coverage for cli/test.py (up from 79.08%)
All tests marked @pytest.mark.unit for fast execution.

Fixes: #14
```

---

## Summary of Changes Required

| File | Action | Lines | Complexity |
|------|--------|-------|------------|
| `tests/unit/cli/test_test.py` | ‚úÖ DONE: Add `import logging` | 28 | Trivial |
| `tests/unit/cli/test_test.py` | üîß FIX: Change caplog level to INFO | 656 | Trivial |
| `tests/unit/cli/test_test.py` | üîß FIX: Add `"type": "logistic_regression"` | 885-891 | Trivial |
| `tests/unit/cli/test_test.py` | ‚ûï ADD: Missing classifier key test | After 868 | Simple (~30 LOC) |
| `tests/unit/cli/test_test.py` | ‚ûï ADD: Corrupt cache test | After new test | Simple (~50 LOC) |
| `src/antibody_training_esm/cli/test.py` | üõ†Ô∏è FIX: Add cache error handling | 261-274 | Medium (~15 LOC) |
| `TEST_COVERAGE_PLAN.md` | üìù UPDATE: Fix issue ref, document results | 236-269 | Simple |

**Total Effort**: ~2-3 hours (including testing and verification)

---

## Acceptance Criteria

Before merging this branch, verify:

- [ ] Import bug fixed (`import logging` added)
- [ ] Device mismatch test passes (caplog level changed to INFO)
- [ ] Hierarchical path test passes (`"type"` field added)
- [ ] Missing classifier key test added and passes
- [ ] Embedding cache error handling added to production code
- [ ] Corrupt cache test added and passes
- [ ] All 32 tests in `test_test.py` PASS
- [ ] Coverage of cli/test.py ‚â•85%
- [ ] `make all` passes (format, lint, typecheck, test)
- [ ] TEST_COVERAGE_PLAN.md updated with Issue #14 results
- [ ] Commit message accurately describes work
- [ ] No coverage regression in other modules

---

## Lessons Learned (For Future Async Agents AND Prompt Writers!)

### ‚ùå What Went Wrong

**Async Agent**:
1. **Incomplete task list verification**: Only implemented 6/8 tasks from Issue #14
2. **Wrong test data**: Used config missing required `"type"` field
3. **Wrong logging level**: Set caplog to WARNING, filtered out INFO assertions
4. **Missing production code**: Didn't notice that embedding cache has no error handling

**Prompt Writer (Me!)**:
1. **Wrong logging level in example**: Specified `caplog.set_level(logging.WARNING)` in prompt, then expected INFO messages
2. **Didn't validate Issue #14 line numbers**: Issue body has wrong line numbers for some tasks
3. **Jumped to conclusions**: Initially claimed device test was impossible based on coverage artifact, didn't verify manually
4. **Over-complicated spec**: First spec was 400+ lines analyzing impossible tests instead of just listing fixes

### ‚úÖ What Went Right

**Async Agent**:
1. **6/8 tests are legitimate and working** - 75% success rate
2. **Good test structure** - AAA pattern, descriptive names, proper docstrings
3. **Real error paths** - Tests exercise real code (for implemented tasks)
4. **No bogus mocking** - Only mocked external I/O as instructed

**Feedback Agent**:
1. **First principles validation** - Manually reproduced tests to verify claims
2. **Caught missing tasks** - Identified 2 tasks not implemented
3. **Accurate diagnosis** - Correctly identified caplog level issue, not impossible test
4. **Found production bug** - Embedding cache has no error handling!

### üìã Improvements for Future

**For Prompt Writers**:
```
CRITICAL VALIDATION RULES:
1. Verify Issue #XX line numbers are accurate before writing examples
2. Test example code snippets for internal consistency (e.g., logging level vs assertions)
3. Check if production code has error handling before requiring tests for it
4. Validate ALL tasks in issue body, not just first few
5. Provide complete, runnable test examples (not pseudocode)
```

**For Async Agents**:
```
CRITICAL VALIDATION RULES:
1. Verify EVERY task in issue body is implemented before claiming completion
2. Run EVERY test individually BEFORE committing
3. Check pytest output for ACTUAL line coverage, not just pass/fail
4. If test data requires specific fields (like "type"), verify against production code
5. If production code has no error handling, note this as blocker (don't skip silently)
```

---

## Approval Checklist

- [ ] Senior developer reviews this REVISED spec
- [ ] Remediation plan approved (2 fixes + 2 new tests + 1 production code fix)
- [ ] Coverage expectations realistic (‚â•85%)
- [ ] Production code fix for embedding cache approved
- [ ] Ready to proceed with implementation

**Status**: üü° **AWAITING SENIOR APPROVAL**

---

## Appendix: Issue #14 Line Number Confusion

The Issue #14 body contains INCORRECT line numbers for some tasks. Here's the mapping:

| Task Description (from Issue) | Issue Says | Actual Lines | Async Agent Status |
|-------------------------------|------------|--------------|-------------------|
| Corrupt model config JSON | 141-171 ‚ùå | 518-520 ‚úÖ | ‚úÖ Implemented |
| Model config missing "model_name" | 141-171 ‚ùå | 495-496, 518-520 ‚úÖ | ‚úÖ Implemented |
| Model config missing "classifier" | 141-171 ‚ùå | 486-520 ‚úÖ | ‚ùå NOT Implemented |
| Embedding cache invalid data | 223-225 ‚ùå | 261-274 ‚úÖ | ‚ùå NOT Implemented (no error handling exists!) |
| Model config not found | 481-518 ‚úÖ | 481-484 ‚úÖ | ‚úÖ Implemented |
| Custom model name format | 481-518 ‚úÖ | 487-516 ‚úÖ | üîß Implemented but broken |

**Lines 141-171**: Actually device mismatch handling (not in original Issue #14 task list, but implicitly required)
**Lines 223-225**: Actually Jain validation (not in original Issue #14 task list, but implicitly required)

This explains why the async agent's work appears incomplete - they followed the ACTUAL code structure instead of the confused line numbers in the issue body.
