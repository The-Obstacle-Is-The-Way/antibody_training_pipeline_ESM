# Boughter Preprocessing Pipeline Reorganization Plan

**Status:** ‚úÖ VALIDATED & READY FOR SENIOR REVIEW
**Date:** 2025-11-04
**Last Updated:** 2025-11-04 (All numbers verified from source code and data files)
**Branch:** `leroy-jenkins/full-send`
**Author:** Ray + Claude Code

---

## üîç VALIDATION SUMMARY

**All claims verified from first principles against actual code and data:**

‚úÖ **Sequence Counts (from preprocessing/process_boughter.py:24-28):**
- Raw FASTA: 1,171 sequences
- Stage 1 output: 1,117 sequences (boughter.csv - VERIFIED)
- Stage 2 output: 1,110 sequences (ANARCI annotation)
- Stage 3 output: 1,065 sequences (16 fragment CSVs - VERIFIED)
- Training subset: 914 sequences (VH_only_boughter_training.csv - VERIFIED)

‚úÖ **File Architecture (from actual file inspection):**
- 16 fragment CSVs contain ALL 1,065 QC-passed sequences
- Each row has `include_in_training` flag (True for 914, False for 151)
- Training CSV is separate file with filtered 914 rows
- Config uses training-specific CSV (configs/config.yaml:13 - VERIFIED)

‚úÖ **Duplicate Scripts (via diff -u):**
- Root and subfolder converter scripts differ by ONE unused `import sys`
- No functional difference - logically identical
- Safe to delete subfolder duplicates

‚úÖ **Current Architecture (from code inspection):**
- process_boughter.py does BOTH Stage 2 and Stage 3 (477 lines total)
- Shared helper functions between stages
- Well-documented with clear stage boundaries

---

## 1. Executive Summary

### Problem Statement
The current Boughter preprocessing pipeline has **duplicate scripts**, unclear organization, and mixes one-off dev scripts with reproducible pipeline components. This violates the principle of having a **single source of truth** for dataset processing.

### Objective
Reorganize Boughter preprocessing into a **clean, self-contained, reproducible pipeline** under `preprocessing/boughter/` with clear naming, proper staging, and removal of duplicates/dead code.

### Guiding Principle
- `preprocessing/` = **REPRODUCIBLE PIPELINES** (single source of truth)
- `scripts/` = **ONE-OFF UTILITIES** (dev tools, experiments, ad-hoc analysis)

---

## 2. Current State Analysis

### 2.1 Boughter File Inventory

#### A. Preprocessing Scripts (1 file)
```
preprocessing/
‚îî‚îÄ‚îÄ process_boughter.py              # Stage 2+3: ANARCI annotation + QC filtering
```

#### B. Scripts (DUPLICATES + SCATTERED)
```
scripts/
‚îú‚îÄ‚îÄ convert_boughter_to_csv.py       # 569 lines - Stage 1 (ROOT LEVEL)
‚îú‚îÄ‚îÄ validate_boughter_conversion.py  # Validation (ROOT LEVEL)
‚îî‚îÄ‚îÄ conversion/
    ‚îî‚îÄ‚îÄ convert_boughter_to_csv.py   # 568 lines - Stage 1 (SUBFOLDER) ‚Üê DUPLICATE!
‚îî‚îÄ‚îÄ validation/
    ‚îî‚îÄ‚îÄ validate_boughter_conversion.py  # Validation (SUBFOLDER) ‚Üê DUPLICATE!
```

**KEY FINDING:** Root-level and subfolder versions differ by ONE unused import
- Root version (569 lines): Has `import sys` on line 16 (UNUSED - no sys calls in code)
- Subfolder version (568 lines): Missing `import sys` import
- **VERIFIED:** This is the ONLY difference (via `diff -u`)
- **CONCLUSION:** NO functional difference - scripts are logically identical
- Root version is newer (commit 8a80408, more recent than subfolder version)

#### C. Test Files (Keep as-is)
```
tests/
‚îî‚îÄ‚îÄ test_boughter_embedding_compatibility.py  # Unit tests for ESM embeddings
```

#### D. Data Files (Organized ‚úì)

**VERIFIED FILE COUNTS AND CONTENTS:**

```
train_datasets/
‚îú‚îÄ‚îÄ boughter.csv                     # Stage 1 output (1,117 sequences)
‚îú‚îÄ‚îÄ boughter/                        # Stage 2+3 outputs (17 CSV files total)
‚îÇ   ‚îú‚îÄ‚îÄ VH_only_boughter.csv            # All QC-passed (1,065 rows, includes include_in_training flag)
‚îÇ   ‚îú‚îÄ‚îÄ VH_only_boughter_training.csv   # ‚≠ê USED BY CONFIG! (914 sequences, training subset only)
‚îÇ   ‚îú‚îÄ‚îÄ H-CDR3_boughter.csv             # All QC-passed (1,065 rows)
‚îÇ   ‚îú‚îÄ‚îÄ [+13 more fragment files...]    # All have 1,065 rows with include_in_training flag
‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ validation_report.txt
‚îÇ   ‚îú‚îÄ‚îÄ annotation_failures.log
‚îÇ   ‚îî‚îÄ‚îÄ qc_filtered_sequences.txt
‚îî‚îÄ‚îÄ boughter_raw/                    # Raw FASTA files (21 files, gitignored)
```

**Key Architecture Detail:**
- Each fragment CSV (16 files) contains ALL 1,065 QC-passed sequences
- Each row has an `include_in_training` flag (True/False) marking training eligibility
- Training subset (914 sequences) = rows where `include_in_training == True`
- `VH_only_boughter_training.csv` is a filtered version containing only the 914 training rows
- Config file uses the training-specific CSV, NOT the full fragment CSV

#### E. Documentation (Well-organized ‚úì)
```
docs/boughter/
‚îú‚îÄ‚îÄ BOUGHTER_P0_FIX_REPORT.md        # References: preprocessing/process_boughter.py
‚îú‚îÄ‚îÄ boughter_cdr_boundary_investigation.md
‚îú‚îÄ‚îÄ boughter_data_sources.md         # Comprehensive methodology doc
‚îú‚îÄ‚îÄ boughter_processing_implementation.md
‚îî‚îÄ‚îÄ boughter_processing_status.md
```

#### F. References in Other Scripts (18 files mention "boughter")
```
scripts/verify_novo_parity.py        # Uses trained model, NOT preprocessing
scripts/testing/*                    # Uses trained model
scripts/analysis/*                   # Analyzes results
experiments/novo_parity/scripts/*    # Research experiments
```

**None of these import or depend on the preprocessing scripts directly.**

---

## 3. Boughter Processing Pipeline Architecture

### 3.1 Current 3-Stage Pipeline

**VERIFIED SEQUENCE COUNTS (from preprocessing/process_boughter.py:24-28):**

```
Raw Data: 1,171 DNA sequences in FASTA format

Stage 1: DNA Translation & Novo Flagging
‚îî‚îÄ Input:  train_datasets/boughter_raw/*.txt (1,171 DNA sequences)
‚îî‚îÄ Script: scripts/convert_boughter_to_csv.py
‚îî‚îÄ Output: train_datasets/boughter.csv (1,117 sequences, 95.4% translation success)
‚îî‚îÄ Losses: 54 sequences (translation failures)

Stage 2: ANARCI Annotation (IMGT numbering)
‚îî‚îÄ Input:  train_datasets/boughter.csv (1,117 sequences)
‚îî‚îÄ Script: preprocessing/process_boughter.py
‚îî‚îÄ Output: Annotated DataFrame (1,110 sequences, 99.4% annotation success)
‚îî‚îÄ Losses: 7 sequences (ANARCI annotation failures)

Stage 3: Post-Annotation Quality Control
‚îî‚îÄ Input:  Annotated DataFrame (1,110 sequences)
‚îî‚îÄ Filter: Remove X in CDRs, empty CDRs
‚îî‚îÄ Script: preprocessing/process_boughter.py (same script as Stage 2)
‚îî‚îÄ Output: train_datasets/boughter/*_boughter.csv (1,065 sequences, 95.9% QC pass)
‚îî‚îÄ Losses: 45 sequences (QC filtering: X in CDRs or empty CDRs)

Training Subset Filtering (Novo Nordisk Methodology):
‚îî‚îÄ Input:  Fragment CSVs (1,065 sequences with include_in_training flag)
‚îî‚îÄ Filter: Keep only 0 flags OR >3 flags (exclude 1-3 flags)
‚îî‚îÄ Output: train_datasets/boughter/VH_only_boughter_training.csv (914 sequences)
‚îî‚îÄ Breakdown: 443 specific (0 flags) + 471 non-specific (>3 flags)
‚îî‚îÄ Excluded: 151 sequences (1-3 flags - mildly polyreactive, per Novo methodology)
```

### 3.2 Key Dependencies

```python
# Stage 1 Dependencies
from Bio import SeqIO           # DNA translation
from Bio.Seq import Seq
import pandas as pd

# Stage 2+3 Dependencies
import riot_na                  # ANARCI wrapper
import pandas as pd
```

### 3.3 Output Used by Training
```yaml
# configs/config.yaml:13
train_file: ./train_datasets/boughter/VH_only_boughter_training.csv
```

**CRITICAL:** Only `train_datasets/boughter/VH_only_boughter_training.csv` is used by the actual training pipeline!

---

## 4. Proposed Reorganization

### 4.1 New Directory Structure

**UPDATED (reflecting keep-combined decision for Stage 2+3):**

```
preprocessing/
‚îî‚îÄ‚îÄ boughter/
    ‚îú‚îÄ‚îÄ README.md                           # Pipeline overview and usage
    ‚îú‚îÄ‚îÄ stage1_dna_translation.py           # Stage 1: DNA ‚Üí protein + Novo flags
    ‚îú‚îÄ‚îÄ stage2_stage3_annotation_qc.py      # Stages 2+3: ANARCI + QC filtering (combined)
    ‚îú‚îÄ‚îÄ validate_stage1.py                  # Validates Stage 1 output (boughter.csv, 1,117 seqs)
    ‚îú‚îÄ‚îÄ validate_stages2_3.py               # Validates Stages 2+3 output (16 fragments, 1,065 seqs)
    ‚îî‚îÄ‚îÄ run_full_pipeline.sh                # Orchestrates all stages (optional)
```

**Note:** Stage 2+3 kept as single script (477 lines) to:
- Avoid code duplication of shared helper functions
- Minimize refactoring risk during initial reorganization
- Maintain current well-tested architecture
- Can be split later if needed

### 4.2 Clear Naming Convention

**BEFORE (confusing):**
- `convert_boughter_to_csv.py` - What does "convert" mean? From what to what?
- `process_boughter.py` - What does "process" mean? All stages?

**AFTER (crystal clear):**
- `stage1_dna_translation.py` - Translates DNA ‚Üí protein, applies Novo flags
- `stage2_anarci_annotation.py` - ANARCI annotation with IMGT numbering
- `stage3_quality_control.py` - Post-annotation QC filtering
- `validate_stage1_output.py` - Validates Stage 1 output
- `validate_stage2_output.py` - Validates Stage 2+3 output

**Benefits:**
1. ‚úì Explicit stage numbers match documentation
2. ‚úì Names describe WHAT the stage does, not generic verbs
3. ‚úì Validation scripts clearly tied to stages
4. ‚úì New contributors can understand flow instantly

---

## 5. Migration Plan

### 5.1 Files to CREATE (New Structure)

**UPDATED (Stage 2+3 combined):**

```bash
preprocessing/boughter/
‚îú‚îÄ‚îÄ README.md                        # NEW - Pipeline documentation
‚îú‚îÄ‚îÄ stage1_dna_translation.py        # REFACTOR from scripts/convert_boughter_to_csv.py (root version)
‚îú‚îÄ‚îÄ stage2_stage3_annotation_qc.py   # MOVE from preprocessing/process_boughter.py (keep combined)
‚îú‚îÄ‚îÄ validate_stage1.py               # REFACTOR from scripts/validate_boughter_conversion.py
‚îî‚îÄ‚îÄ validate_stages2_3.py            # NEW - Validates fragment CSVs (extract from validate_fragments.py)
```

### 5.2 Files to DELETE (Duplicates + Dead Code)

```bash
# Root-level duplicates (OLDER versions)
scripts/convert_boughter_to_csv.py           ‚Üê DELETE (duplicate, older)
scripts/validate_boughter_conversion.py      ‚Üê DELETE (duplicate, older)

# Subfolder versions (may have minor diffs, will merge into preprocessing)
scripts/conversion/convert_boughter_to_csv.py     ‚Üê DELETE after migration
scripts/validation/validate_boughter_conversion.py ‚Üê DELETE after migration

# Old preprocessing script (will be split)
preprocessing/process_boughter.py            ‚Üê DELETE after split into stage2 + stage3
```

### 5.3 Files to UPDATE (Documentation References)

```bash
docs/boughter/BOUGHTER_P0_FIX_REPORT.md:199
‚îî‚îÄ OLD: python3 preprocessing/process_boughter.py
‚îî‚îÄ NEW: python3 preprocessing/boughter/stage2_anarci_annotation.py

docs/boughter/boughter_processing_status.md
‚îî‚îÄ Update all script references

README.md (if exists)
‚îî‚îÄ Update pipeline instructions
```

---

## 6. Detailed Refactoring Steps

### Step 1: Create Directory Structure
```bash
mkdir -p preprocessing/boughter
```

### Step 2: Migrate Stage 1 (DNA Translation)
```bash
# Source: scripts/convert_boughter_to_csv.py (ROOT version - 569 lines, newer)
# Target: preprocessing/boughter/stage1_dna_translation.py

# Changes:
# 1. Keep all logic exactly as-is
# 2. Add comprehensive docstring with stage overview
# 3. Update output path documentation
# 4. Add input/output validation
```

### Step 3: Split process_boughter.py into Stage 2 + Stage 3
```bash
# Source: preprocessing/process_boughter.py (293 lines)
# Target: preprocessing/boughter/stage2_anarci_annotation.py
#         preprocessing/boughter/stage3_quality_control.py

# Stage 2 (ANARCI Annotation):
# - Extract annotate_sequence() function (lines 44-141)
# - Extract ANARCI processing logic (Stage 2 only)
# - Stop after creating annotated DataFrame

# Stage 3 (QC Filtering):
# - Extract QC filtering logic
# - Apply X-in-CDR and empty-CDR filters
# - Generate 16 fragment-specific CSVs
# - Create validation reports
```

### Step 4: Create Validation Scripts
```bash
# validate_stage1_output.py
# - Refactor from scripts/validate_boughter_conversion.py
# - Validate boughter.csv structure
# - Check sequence counts, translation quality

# validate_stage2_output.py
# - Extract from process_boughter.py validation logic
# - Validate fragment CSV structure
# - Check row counts, label distribution
# - Verify all 16 fragments created
```

### Step 5: Create README.md
```markdown
# Boughter Dataset Preprocessing Pipeline

## Overview
3-stage pipeline for processing Boughter 2020 dataset according to Novo Nordisk methodology.

## Usage
```bash
# Stage 1: DNA Translation & Novo Flagging
python3 preprocessing/boughter/stage1_dna_translation.py

# Validate Stage 1
python3 preprocessing/boughter/validate_stage1_output.py

# Stage 2: ANARCI Annotation
python3 preprocessing/boughter/stage2_anarci_annotation.py

# Stage 3: Quality Control & Fragment Extraction
python3 preprocessing/boughter/stage3_quality_control.py

# Validate Stage 2+3
python3 preprocessing/boughter/validate_stage2_output.py
```

## Pipeline Flow
[Stage 1] ‚Üí boughter.csv (1,065 sequences)
[Stage 2] ‚Üí Annotated data (1,110 sequences)
[Stage 3] ‚Üí 16 fragment CSVs (914 training sequences)

## Documentation
See docs/boughter/ for detailed methodology and validation reports.
```

### Step 6: Delete Old Files
```bash
git rm scripts/convert_boughter_to_csv.py
git rm scripts/validate_boughter_conversion.py
git rm scripts/conversion/convert_boughter_to_csv.py
git rm scripts/validation/validate_boughter_conversion.py
git rm preprocessing/process_boughter.py
```

### Step 7: Update Documentation
```bash
# Update all references in docs/boughter/*.md
# Search-replace: preprocessing/process_boughter.py ‚Üí preprocessing/boughter/stage2_anarci_annotation.py
```

### Step 8: Commit Strategy
```bash
# Commit 1: Create new structure
git add preprocessing/boughter/
git commit -m "feat: Create organized Boughter preprocessing pipeline structure"

# Commit 2: Delete old files
git rm scripts/convert_boughter_to_csv.py ...
git commit -m "refactor: Remove duplicate and legacy Boughter preprocessing scripts"

# Commit 3: Update documentation
git add docs/boughter/
git commit -m "docs: Update Boughter preprocessing references to new structure"
```

---

## 7. Risk Assessment

### 7.1 SAFE Changes (Zero Risk)
- ‚úÖ Creating new `preprocessing/boughter/` directory
- ‚úÖ Copying scripts to new location
- ‚úÖ Deleting duplicate scripts in `scripts/`
- ‚úÖ Updating documentation references

**Why Safe:** No production code imports these scripts directly. They're run manually via command line.

### 7.2 MEDIUM Risk Changes (Require Testing)
- ‚ö†Ô∏è Splitting `process_boughter.py` into Stage 2 + Stage 3
- ‚ö†Ô∏è Refactoring validation logic

**Mitigation:**
1. Keep original files until new pipeline validated
2. Run full pipeline and compare outputs byte-by-byte
3. Verify training still works with new fragment CSVs

### 7.3 ZERO RISK to Production
- ‚úÖ Training uses `train_datasets/boughter/VH_only_boughter_training.csv` (already generated)
- ‚úÖ No imports of preprocessing scripts in production code
- ‚úÖ Changes are purely organizational, not functional

---

## 8. Validation Checklist

### Before Migration
- [ ] Verify current pipeline produces correct outputs
- [ ] Document current output file hashes/sizes
- [ ] Run training with current data and record metrics

### After Migration
- [ ] Run Stage 1, compare `boughter.csv` with original (should be identical)
- [ ] Run Stage 2+3, compare fragment CSVs with originals (should be identical)
- [ ] Verify 914 sequences in `VH_only_boughter_training.csv`
- [ ] Run training with new data, verify metrics match
- [ ] Verify all 16 fragment files generated
- [ ] Check validation reports created

### Documentation
- [ ] All `docs/boughter/*.md` references updated
- [ ] `preprocessing/boughter/README.md` created
- [ ] Pipeline usage instructions clear and tested

---

## 9. Future Extensibility

### Why This Structure is Better

**OLD (scattered):**
```
scripts/convert_boughter_to_csv.py      # Hard to find
scripts/conversion/convert_boughter_to_csv.py  # Wait, which one?
preprocessing/process_boughter.py       # Does this do Stage 1 or 2 or both?
```

**NEW (organized):**
```
preprocessing/boughter/
‚îú‚îÄ‚îÄ stage1_dna_translation.py      # Crystal clear
‚îú‚îÄ‚îÄ stage2_anarci_annotation.py    # No ambiguity
‚îú‚îÄ‚îÄ stage3_quality_control.py      # Self-documenting
‚îî‚îÄ‚îÄ validate_stage1_output.py      # Obvious purpose
```

### Applying to Jain/Harvey/Shehata

Once Boughter is reorganized, we can apply the same pattern:

```
preprocessing/
‚îú‚îÄ‚îÄ boughter/
‚îÇ   ‚îú‚îÄ‚îÄ stage1_*.py
‚îÇ   ‚îú‚îÄ‚îÄ stage2_*.py
‚îÇ   ‚îî‚îÄ‚îÄ stage3_*.py
‚îú‚îÄ‚îÄ jain/
‚îÇ   ‚îú‚îÄ‚îÄ stage1_excel_conversion.py
‚îÇ   ‚îú‚îÄ‚îÄ stage2_anarci_annotation.py
‚îÇ   ‚îú‚îÄ‚îÄ stage3_quality_control.py
‚îÇ   ‚îî‚îÄ‚îÄ stage4_p5e_s2_filtering.py     # Dataset-specific
‚îú‚îÄ‚îÄ harvey/
‚îÇ   ‚îî‚îÄ‚îÄ stage1_nanobody_processing.py  # Different pipeline (nanobodies)
‚îî‚îÄ‚îÄ shehata/
    ‚îî‚îÄ‚îÄ stage1_excel_conversion.py
```

**Benefits:**
- ‚úì Each dataset has self-contained pipeline
- ‚úì Stage numbers make execution order obvious
- ‚úì Dataset-specific quirks isolated to their folders
- ‚úì New contributors understand structure instantly

---

## 10. Open Questions for Senior Review

### Q1: Stage Splitting Strategy

**Current Architecture (VERIFIED from preprocessing/process_boughter.py:1-477):**
- `process_boughter.py` performs BOTH Stage 2 (ANARCI annotation) AND Stage 3 (QC filtering) in a single 477-line script
- Both stages share common helper functions (`annotate_sequence()`, `process_antibody()`, etc.)
- Current flow: Read ‚Üí Annotate (Stage 2) ‚Üí Filter QC (Stage 3) ‚Üí Create 16 fragments ‚Üí Done

**Options:**
- **A) SPLIT:** `stage2_anarci_annotation.py` + `stage3_quality_control.py` (more modular, clearer stages)
  - **Pros:** Crystal clear stage separation, easier to understand pipeline flow
  - **Cons:** Need to create shared helper module or duplicate helper functions, more files to test

- **B) KEEP COMBINED:** `stage2_stage3_annotation_and_qc.py` (current architecture, less refactoring)
  - **Pros:** No code duplication, maintains current helper function structure, less testing risk
  - **Cons:** One large file (477 lines), stage boundaries less explicit in file structure

**Recommendation:** Option B (keep combined) for INITIAL reorganization to minimize risk. Can split later if needed. The combined script is well-documented with clear stage comments (lines 10-11, 346-347).

### Q2: Validation Script Naming
**Current:** `validate_boughter_conversion.py` (vague)

**Options:**
- **A)** `validate_stage1_output.py` + `validate_stage2_output.py` (stage-specific)
- **B)** `validate_boughter_pipeline.py` (single comprehensive validator)

**Recommendation:** Option A (stage-specific) to catch issues early in pipeline.

### Q3: Root-Level vs Subfolder Duplicates (RESOLVED)

**Finding (VERIFIED via diff -u):**
- Root-level `convert_boughter_to_csv.py` has ONE extra line: `import sys` (line 16)
- This import is UNUSED - no `sys.*` calls anywhere in the code
- No other differences in logic, functions, or behavior

**Question:** Which version is the "correct" one to migrate?

**Answer:** Root-level version (commit 8a80408, newer). The `import sys` difference is cosmetic only - no functional impact. Subfolder version can be safely deleted without losing any functionality.

### Q4: Backwards Compatibility
**Question:** Should we keep symlinks for old script paths during transition?

**Recommendation:** NO - clean break is better. No production code depends on these paths.

---

## 11. Estimated Effort

### Time Breakdown
- **Planning:** 1 hour (DONE - this doc)
- **Senior Review:** 30 min (pending)
- **Implementation:** 2-3 hours
  - Create new structure: 30 min
  - Migrate Stage 1: 30 min
  - Split Stage 2+3: 60 min
  - Create validation scripts: 30 min
  - Testing: 30 min
- **Documentation Updates:** 30 min
- **Final Validation:** 30 min

**Total:** ~4 hours (half a workday)

### Rollback Plan
If anything breaks:
```bash
git revert <commit-hash>
# Or restore from ray/dev backup branch
```

**Data Safety:** All generated data files (`train_datasets/boughter/`) are unchanged - only script organization changes.

---

## 12. Success Criteria

### Must Have ‚úÖ
- [ ] Single source of truth: `preprocessing/boughter/` contains all pipeline code
- [ ] Clear stage naming: `stage1_*.py`, `stage2_*.py`, `stage3_*.py`
- [ ] No duplicate scripts remaining in `scripts/`
- [ ] All documentation updated with new paths
- [ ] Pipeline produces identical outputs to current version
- [ ] Training works with reorganized pipeline

### Nice to Have üéØ
- [ ] `run_full_pipeline.sh` orchestration script
- [ ] Comprehensive `preprocessing/boughter/README.md`
- [ ] Validation scripts with detailed error messages

---

## 13. Next Steps

1. **SENIOR REVIEW** (REQUIRED BEFORE PROCEEDING)
   - Review this plan for correctness
   - Approve organizational strategy
   - Resolve open questions (Section 10)

2. **Implementation** (After approval)
   - Execute migration steps (Section 6)
   - Run validation checklist (Section 8)
   - Update documentation (Section 6, Step 7)

3. **Apply to Other Datasets** (Future)
   - Jain (most complex - 4 stages)
   - Harvey (nanobodies - different pipeline)
   - Shehata (similar to Jain)

---

## 14. Reviewer Checklist

**For Senior Engineer Review:**

- [ ] Organizational strategy makes sense (preprocessing/ vs scripts/)
- [ ] Stage naming convention is clear and maintainable
- [ ] Risk assessment is accurate (Section 7)
- [ ] Migration plan is complete (Section 6)
- [ ] Validation strategy is sufficient (Section 8)
- [ ] Open questions resolved (Section 10)
- [ ] Backwards compatibility concerns addressed
- [ ] Effort estimate is reasonable (Section 11)

**Sign-off:**
- Reviewer: _______________
- Date: _______________
- Decision: [ ] APPROVED  [ ] NEEDS CHANGES  [ ] REJECTED

---

## Appendix A: File Comparison

### convert_boughter_to_csv.py Differences

```bash
# Root version (scripts/convert_boughter_to_csv.py) - 569 lines
16: import sys                  # ‚Üê PRESENT

# Subfolder version (scripts/conversion/convert_boughter_to_csv.py) - 568 lines
16: [missing]                   # ‚Üê ABSENT
```

**Conclusion:** Root version is more complete. Use as migration source.

### validate_boughter_conversion.py Differences

```bash
# Need to run detailed diff
diff scripts/validate_boughter_conversion.py scripts/validation/validate_boughter_conversion.py
```

**Action:** Manual review required to merge any unique logic from both versions.

---

## Appendix B: Current Pipeline Execution

```bash
# Current command sequence (manual, undocumented)
python3 scripts/convert_boughter_to_csv.py
python3 preprocessing/process_boughter.py
python3 scripts/validate_boughter_conversion.py  # (Optional validation)

# Proposed command sequence (clear, documented)
python3 preprocessing/boughter/stage1_dna_translation.py
python3 preprocessing/boughter/validate_stage1_output.py
python3 preprocessing/boughter/stage2_anarci_annotation.py
python3 preprocessing/boughter/stage3_quality_control.py
python3 preprocessing/boughter/validate_stage2_output.py
```

---

**END OF PLAN**

**STATUS:** üö® AWAITING SENIOR REVIEW - DO NOT PROCEED WITHOUT APPROVAL
