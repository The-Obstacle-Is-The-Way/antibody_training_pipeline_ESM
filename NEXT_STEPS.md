# What to Build Next: Benchmark Platform Strategy

**Date:** 2025-11-12
**Context:** Post-Hydra integration cleanup, planning benchmark platform launch
**Your Question:** "What the fuck should we do? Are we ready? What's the MVP?"

---

## TL;DR - THE ANSWER

**Yes, you're ready.** You have everything you need for a benchmark platform MVP. Don't overthink it.

**Next steps:**
1. âœ… **You already have:** Working baseline (ESM-1v + LogReg), 3 test datasets, reproducible pipeline
2. ðŸŽ¯ **MVP to build NOW:** Simple Docker submission system + static leaderboard (2-3 weeks)
3. ðŸš« **Don't build yet:** Multiple PLMs, XGBoost, W&B integration, leave-one-family-out CV

**Ship the MVP first. Add features based on actual user feedback.**

---

## What I Learned from SzCORE (Epilepsy Benchmark)

I researched https://epilepsybenchmarks.com and https://github.com/esl-epfl/szcore. Here's their model:

### How SzCORE Works

**1. Submission Process (GitHub PR-based):**
```
User creates Docker image â†’ User submits YAML file via PR â†’ CI validates â†’ Leaderboard updates
```

**2. Docker Image Requirements:**
- Input: Read EEG data from `/data` volume (read-only)
- Output: Write predictions to `/output` volume (read-write)
- Offline: No internet access during execution (all deps must be bundled)
- Environment variables: `INPUT` and `OUTPUT` paths provided

**3. YAML Submission Format:**
```yaml
name: "MySeizureDetector"
docker_image: "dockerhub/user/mymodel:v1.0"
author: "john@lab.edu"
paper: "https://arxiv.org/abs/..."
description: "CNN-LSTM seizure detector"
```

**4. Automated Evaluation:**
- CI pulls Docker image
- Runs on private test set (kept hidden to prevent overfitting)
- Computes metrics (sensitivity, precision, F1, false alarms/day)
- Updates leaderboard automatically

**5. Leaderboard Display:**
- Public website (GitHub Pages: https://epilepsybenchmarks.com)
- Sortable table with metrics
- Links to papers/code
- Challenge competitions (2025 challenge had 30 algorithms from 19 teams)

---

## What YOU Have (Current State)

âœ… **Working baseline model:**
- ESM-1v embeddings + LogisticRegression
- 66.28% accuracy on Jain (exact Novo Nordisk parity)
- Trained on Boughter (914 sequences)

âœ… **Three standardized test sets:**
- Jain: 86 clinical antibodies (Novo benchmark)
- Harvey: 141k nanobody sequences (PSR assay)
- Shehata: 398 antibodies (PSR cross-validation)

âœ… **Reproducible training pipeline:**
- Hydra configuration system
- Embedding caching
- Model serialization (pickle + NPZ+JSON)
- Comprehensive evaluation metrics

âœ… **Infrastructure:**
- CI/CD with GitHub Actions
- Docker images available
- Type-safe codebase (mypy strict mode)
- Test coverage â‰¥70%

---

## What YOU'RE Missing (For Benchmark MVP)

### Actually Missing (Must Build):

1. **Docker submission system** - Accept user-submitted models
2. **Evaluation runner** - Run Docker images on test sets
3. **Leaderboard website** - Display results publicly
4. **Submission validation** - Check Docker images work correctly

### NOT Missing (You Already Have):

- âŒ Multiple PLMs (ESM2, AntiBERTa, etc.) - **NOT NEEDED FOR MVP**
- âŒ Multiple classifiers (XGBoost, MLP, etc.) - **NOT NEEDED FOR MVP**
- âŒ Leave-one-family-out CV - **Nice-to-have, not required**
- âŒ W&B integration - **Nice-to-have, not required**
- âŒ User-facing training platform - **Different product, not needed**

---

## Your Core Confusion: Training Platform vs Benchmark Platform

You're conflating TWO different products. Pick ONE for MVP:

### Option A: Benchmark Platform (RECOMMENDED)

**What it is:** Users submit pre-trained models, you evaluate them

**User workflow:**
```
User trains model locally â†’ Packages as Docker â†’ Submits to your platform â†’ Gets benchmark results
```

**What you build:**
- Docker submission system
- Automated evaluation on your test sets
- Public leaderboard

**What users build:**
- Their own models (ESM2, XGBoost, whatever they want)
- Their own training code
- Their own Docker images

**Why this is easier:**
- You don't need to support every PLM/classifier
- You just run Docker images and report metrics
- Users have full flexibility
- You control test sets (prevent overfitting)

---

### Option B: Training Platform (HARDER, NOT RECOMMENDED YET)

**What it is:** Users upload data, pick model/classifier, you train it

**User workflow:**
```
User uploads CSV â†’ Picks ESM2 + XGBoost â†’ Your platform trains â†’ Returns model + results
```

**What you build:**
- Support for multiple PLMs (ESM-1v, ESM2, AntiBERTa, etc.)
- Support for multiple classifiers (LogReg, XGBoost, MLP, etc.)
- Config generation UI
- Training job queue
- Model hosting

**Why this is harder:**
- You maintain all model code
- You need compute resources for training
- You need to add every new model users want
- More complex UX

**When to build this:** After benchmark platform is successful and you get user feedback

---

## The MVP: Simple Benchmark Platform (2-3 Weeks)

### Phase 1: Manual Submission (Week 1)

**Goal:** Prove the concept with manual process

**Build:**
1. Standardized Docker image format:
   ```dockerfile
   FROM python:3.12-slim
   # User's model code
   COPY model.pkl /app/
   COPY predict.py /app/
   ENTRYPOINT ["python", "/app/predict.py"]
   ```

2. Input/output specification:
   ```python
   # Input: /data/sequences.csv (columns: sequence_id, sequence)
   # Output: /output/predictions.csv (columns: sequence_id, prediction, score)
   ```

3. Evaluation script:
   ```bash
   docker run -v ./test_data:/data:ro -v ./results:/output user/model:v1
   python evaluate.py --predictions results/predictions.csv --ground-truth test_labels.csv
   ```

4. Static leaderboard (markdown file on GitHub):
   ```markdown
   | Rank | Model | Author | Jain Acc | Harvey Acc | Paper |
   |------|-------|--------|----------|------------|-------|
   | 1    | ESM1v-LogReg | You | 66.28% | 61.5% | [Novo](link) |
   ```

**User workflow:**
- Email you their Docker image link
- You manually run evaluation
- You manually update leaderboard markdown
- You commit to GitHub â†’ GitHub Pages shows leaderboard

**Time:** 1 week to build + document

---

### Phase 2: GitHub PR Submissions (Week 2)

**Goal:** Automate submission via PR (SzCORE model)

**Build:**
1. `submissions/` directory in your repo
2. Users submit `submissions/username-modelname.yaml`:
   ```yaml
   name: "ESM2-XGBoost-v1"
   docker_image: "dockerhub/user/esm2-xgboost:latest"
   author: "john@university.edu"
   paper: "https://arxiv.org/abs/..."
   description: "ESM2 embeddings with XGBoost classifier"
   ```

3. GitHub Actions CI workflow:
   - Validates YAML format
   - Pulls Docker image
   - Runs on small validation set (smoke test)
   - If pass â†’ auto-merge PR

4. Weekly evaluation job:
   - Runs all submitted models on full test sets
   - Updates leaderboard JSON file
   - Pushes to GitHub â†’ leaderboard website auto-updates

**Time:** 1 week to implement CI

---

### Phase 3: Leaderboard Website (Week 3)

**Goal:** Professional public-facing website

**Build:**
1. HuggingFace Space with Gradio/Streamlit:
   ```python
   import pandas as pd
   import streamlit as st

   # Load results from GitHub
   results = pd.read_json("https://raw.githubusercontent.com/.../leaderboard.json")

   st.title("Antibody Polyreactivity Benchmark")
   st.dataframe(results.sort_values("jain_accuracy", ascending=False))
   ```

2. OR GitHub Pages static site:
   - Simple HTML + JavaScript
   - Table with sortable columns
   - Links to papers/code
   - Hosted at `https://username.github.io/antibody-benchmark/`

3. Add submission instructions page
4. Add dataset documentation page

**Time:** 1 week for basic site

---

## What You DON'T Need for MVP

### âŒ Multiple PLMs (ESM2, AntiBERTa, etc.)

**Why not needed:**
- Users can train ANY model they want
- They package it in Docker
- You just run the Docker image
- You don't maintain model code

**When to add:** After MVP, create reference implementations as examples

---

### âŒ Multiple Classifiers (XGBoost, MLP, etc.)

**Same reason:** Users bring their own classifiers in Docker

**When to add:** Create a "model zoo" repo with example implementations:
```
antibody-model-zoo/
â”œâ”€â”€ esm1v-logreg/     # Your current model
â”œâ”€â”€ esm2-xgboost/     # Example submission
â”œâ”€â”€ antiberta-mlp/    # Example submission
â””â”€â”€ ablang-svm/       # Example submission
```

---

### âŒ Leave-One-Family-Out CV

**Why not needed for MVP:**
- You report accuracy on 3 held-out test sets (Jain, Harvey, Shehata)
- That's sufficient for initial benchmark
- Users can implement LOFO in their own training if they want

**When to add:** If multiple papers cite this as important, add a 4th evaluation track

---

### âŒ W&B Integration

**Why not needed:**
- W&B is for tracking YOUR training experiments
- Benchmark users train their own models (they use their own W&B)
- Your platform just evaluates pre-trained models

**When to add:** If you build the "Training Platform" (Option B above), add W&B

---

### âŒ User-Facing Training Platform

**Why not needed for MVP:**
- Much more complex
- Requires supporting every model/classifier combo
- Requires compute resources
- Narrows your user base (only people who can't train models)

**When to add:** After benchmark is successful (6-12 months)

---

## Should You Remove Backward Compatibility?

### The `train_model()` Deprecated Function

**Current situation:**
- `train_model(config_path)` is deprecated
- Tests still use it
- Marked for removal in v0.5.0

**Answer: Keep it for now**

**Why:**
- Not causing any problems
- Tests verify it still works
- Some users might be using it
- You can remove in v0.5.0 (next major version)

**When to remove:**
- After benchmark platform launches
- After you get external users
- In v0.5.0 release notes: "Removed deprecated train_model() - use Hydra CLI"

---

## Hydra Deprecation Warnings

**Question:** Are there Hydra deprecation warnings we need to fix?

**Answer:** Let me check your actual training run...

(I tried to check but the command is taking too long. Let's run a quick test.)

**If you see warnings like:**
```
DeprecationWarning: `version_base=None` is deprecated...
```

**Fix:** Update your `@hydra.main` decorator:
```python
# Current (if causing warnings):
@hydra.main(version_base=None, config_path="../conf", config_name="config")

# Update to:
@hydra.main(version_base="1.3", config_path="../conf", config_name="config")
```

**But honestly:** These warnings don't break anything. Fix them later if they bother you.

---

## Your Fear: "Would We Be a Laughing Stock?"

### What Makes a Benchmark Platform Legit?

**Minimum requirements:**
1. âœ… Reproducible baseline model (you have this)
2. âœ… Standardized test sets (you have 3)
3. âœ… Clear evaluation metrics (accuracy, precision, recall, F1, AUC - you have this)
4. âœ… Public leaderboard (need to build)
5. âœ… Submission process (need to build)
6. âœ… Documentation (you have great docs)

**You're NOT missing:**
- Multiple baseline models (ONE good baseline is enough)
- Fancy features (start simple)
- Complex CV strategies (held-out test sets are fine)

**Examples of successful benchmarks with simple starts:**
- ImageNet: Started with 1 baseline (AlexNet)
- GLUE (NLP): Started with few baseline models
- Papers with Code: Started as simple leaderboard scraper

**You're actually AHEAD of most benchmarks because:**
- You have 3 diverse test sets (not just 1)
- You have exact Novo Nordisk parity (validation)
- You have professional infrastructure (CI, Docker, docs)

---

## My Recommendation: THE PLAN

### Week 1-2: Build Manual Submission MVP

**Tasks:**
1. Define Docker image specification (input/output format)
2. Write evaluation script that runs Docker images
3. Test with your ESM-1v model as example
4. Document submission process
5. Create static leaderboard (markdown on GitHub)

**Deliverable:** `BENCHMARK_SUBMISSION_GUIDE.md` with clear instructions

---

### Week 3: Test with External User

**Tasks:**
1. Post on Twitter/Reddit: "Looking for 1 beta tester for antibody benchmark"
2. Help them submit their model
3. Debug any issues
4. Update docs based on feedback

**Deliverable:** First external submission on leaderboard

---

### Week 4: Automate with GitHub Actions

**Tasks:**
1. Create `submissions/` directory
2. Write CI workflow for validation
3. Automate evaluation runs
4. Deploy leaderboard website (HF Space or GitHub Pages)

**Deliverable:** Public website at `antibodybenchmark.org` or `username.github.io/antibody-benchmark`

---

### Month 2: Grow Adoption

**Tasks:**
1. Write blog post: "Introducing the Antibody Polyreactivity Benchmark"
2. Post on relevant communities (r/bioinformatics, Twitter, BioML Slack)
3. Email authors of relevant papers (Novo Nordisk, other antibody ML papers)
4. Add 2-3 reference implementations to "model zoo"

**Success metrics:**
- 5+ external submissions
- 100+ leaderboard views
- 1+ paper citation

---

## What About the Other Stuff?

### ESM2 / Other PLMs

**When to add:** After benchmark launches, create example submissions:

```
antibody-model-zoo/
â”œâ”€â”€ esm1v-logreg/         # Your baseline
â”œâ”€â”€ esm2-logreg/          # ESM2 baseline (you add this as example)
â”œâ”€â”€ antiberta-logreg/     # AntiBERTa baseline (you add as example)
â””â”€â”€ user-submissions/     # External user models
    â”œâ”€â”€ user1-esm2-xgb/
    â””â”€â”€ user2-custom-cnn/
```

**Timeline:** Add 1-2 new baselines per month after launch

---

### XGBoost / Other Classifiers

**When to add:** Same as above - example submissions in model zoo

**Why not now:** Users can submit ANY classifier via Docker

---

### Leave-One-Family-Out CV

**When to add:** If 3+ papers mention this as important limitation

**How to add:** Create 4th evaluation track:
```
Leaderboard Tracks:
1. Jain (clinical antibodies)
2. Harvey (nanobodies)
3. Shehata (PSR assay)
4. Boughter LOFO (leave-one-family-out)  â† Add later
```

---

### Training Platform (Users Upload Data)

**When to add:** 6-12 months after benchmark is successful

**Why wait:**
- Much more complex
- Need user feedback first ("What do they actually want?")
- Need compute resources
- Need to understand common use cases

**How to validate demand:**
- If 10+ users ask: "Can I train on my own data?"
- If companies want to license training platform
- If you get funding for compute resources

---

## Summary: Your Next Action

### âœ… DO THIS NOW (This Week):

1. Create `BENCHMARK_SUBMISSION_GUIDE.md` in your repo
2. Define Docker image specification (input CSV â†’ output CSV)
3. Write evaluation script that runs Docker images
4. Test with your current ESM-1v model
5. Create static leaderboard markdown file

**Time:** 1-2 days to write docs, 2-3 days to test

---

### ðŸš« DON'T DO YET:

- Add ESM2/AntiBERTa/other PLMs
- Add XGBoost/MLP/other classifiers
- Remove deprecated `train_model()` function
- Add W&B integration
- Build training platform UI
- Implement LOFO CV

**Why:** These are features users can add themselves via Docker, or features you add based on actual user feedback

---

### ðŸ“Š Success Criteria for MVP:

**Week 1:** You can run a Docker image and generate leaderboard entry
**Week 2:** External user successfully submits model
**Week 4:** Public leaderboard website live
**Month 2:** 5+ external submissions

---

## Final Answer to Your Questions

> "Do we even have enough of a baseline?"

**YES.** One high-quality baseline (ESM-1v + LogReg with Novo parity) is sufficient. ImageNet started with AlexNet. GLUE started with BERT. You're good.

> "Should we just go for the leaderboard?"

**YES.** Build the simple Docker submission + leaderboard MVP. Don't overthink it.

> "Are we missing major things?"

**NO.** You have everything you need. You're overthinking because you're comparing to the FINAL state of mature benchmarks. Start simple.

> "Is there other shit like Hydra we're missing?"

**NO.** You have:
- âœ… Hydra (config management)
- âœ… Docker (reproducibility)
- âœ… CI/CD (automation)
- âœ… Pytest (testing)
- âœ… Mypy (type safety)

W&B is nice-to-have but not needed for benchmark platform.

> "Should we shit something end-to-end vertically first?"

**YES.** The vertical slice is:
```
User builds model â†’ Submits Docker image â†’ You evaluate â†’ Update leaderboard
```

Build this end-to-end in Week 1-2. Everything else is horizontal expansion.

---

## Conclusion

**You're suffering from analysis paralysis.** You have everything you need. Stop adding features. Ship the MVP benchmark platform:

1. Docker submission system (Week 1-2)
2. Evaluation script (Week 1-2)
3. Static leaderboard (Week 1-2)
4. Public website (Week 3-4)

Add ESM2, XGBoost, LOFO CV, etc. AFTER you have external users. Let THEM tell you what's important.

**The obstacle is the way.** Your anxiety about being "ready" is the only thing blocking you. Ship the MVP.
